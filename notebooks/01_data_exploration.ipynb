{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61a92935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/raw/Observations 2012-2025.xlsx\n",
      "Loaded 86 species\n",
      "Loaded 651 GPS points\n",
      "Converting count columns to numeric\n",
      "Loaded 114497 observation records\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "%matplotlib inline \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from data_io import load_excel_data, clean_observations, get_annual_summary\n",
    "\n",
    "# Plotting style setup\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Colour palette\n",
    "COLORS = {\n",
    "    'primary': '#2E86AB',\n",
    "    'secondary': '#A23B72',\n",
    "    'accent': '#F18F01',\n",
    "    'success': '#06A77D',\n",
    "    'warning': '#D62246',\n",
    "}\n",
    "\n",
    "# load the data\n",
    "data = load_excel_data('../data/raw/Observations 2012-2025.xlsx')\n",
    "df_obs = data['observations']\n",
    "df_species = data['species']\n",
    "df_gps = data['gps']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910e293",
   "metadata": {},
   "source": [
    "## Survey Protocol\n",
    "\n",
    "This dataset follows a standardized breeding bird monitoring protocol:\n",
    "\n",
    "- **Transect structure**: Each site contains **10 fixed observation points** arranged along a transect\n",
    "- **Visit protocol**: Observers conduct **5-minute point counts** at each of the 10 points\n",
    "- **Visitation frequency**: Each site is visited **at most twice** during the monitoring period\n",
    "- **Detection recording**: All bird detections include species ID, count, and observer metadata\n",
    "\n",
    "This standardized approach ensures comparable data across sites and years, though sampling effort varies (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9801d787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Structure\n",
      "\n",
      "                 Sheet Name   Rows  Columns                           Description\n",
      "Observations (NOM FRANÇAIS) 114497       28  Bird observation records (main data)\n",
      "          Species (ESPECES)     86        3 Species taxonomy and migration status\n",
      "           GPS (GPS-MILIEU)    651        4     GPS coordinates and habitat types\n",
      "Data Cleaning\n",
      "Starting with 114497 records\n",
      "⚠ Found 5 negative wind values - setting to NaN\n",
      "Removed 4 records with zero/negative counts\n",
      "Final dataset: 114493 records\n",
      "  Years: 2014 - 2025\n",
      "  Unique species: 102\n",
      "  Unique transects: 72\n",
      "  Unique observers: 42\n",
      "\n",
      "Total observation records after cleaning: 114,493\n"
     ]
    }
   ],
   "source": [
    "# Dataset Structure Summary Table\n",
    "print(\"Dataset Structure\\n\")\n",
    "\n",
    "sheet_summary = pd.DataFrame({\n",
    "    'Sheet Name': ['Observations (NOM FRANÇAIS)', 'Species (ESPECES)', 'GPS (GPS-MILIEU)'],\n",
    "    'Rows': [len(df_obs), len(df_species), len(df_gps)],\n",
    "    'Columns': [df_obs.shape[1], df_species.shape[1], df_gps.shape[1]],\n",
    "    'Description': [\n",
    "        'Bird observation records (main data)',\n",
    "        'Species taxonomy and migration status',\n",
    "        'GPS coordinates and habitat types'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(sheet_summary.to_string(index=False))\n",
    "print(f\"\\nTotal observation records after cleaning: {len(clean_observations(df_obs)):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a598566e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaning\n",
      "Starting with 114497 records\n",
      "⚠ Found 5 negative wind values - setting to NaN\n",
      "Removed 4 records with zero/negative counts\n",
      "Final dataset: 114493 records\n",
      "  Years: 2014 - 2025\n",
      "  Unique species: 102\n",
      "  Unique transects: 72\n",
      "  Unique observers: 42\n"
     ]
    }
   ],
   "source": [
    "# clean it up\n",
    "df_clean = clean_observations(df_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a1ab6b",
   "metadata": {},
   "source": [
    "### Data Quality Assessment\n",
    "\n",
    "Before proceeding with analyses, we systematically assess data quality to identify issues that may affect downstream results. For each issue,  we first present quantitative evidence, then provide interpretation and document our approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9db7285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Analysis\n",
      "\n",
      "1. Temporal Coverage Analysis\n",
      "\n",
      "Statistical threshold for completeness:\n",
      "Mean observations (2015-2024): 9,934\n",
      "Standard deviation: 835\n",
      "Threshold (mean - 3×SD): 7,430\n",
      "\n",
      "Partial years identified: 2014\n",
      "  2014: 5,377 obs (54% of typical year)\n",
      "\n",
      "Interpretation:\n",
      "2014 is partial (significantly fewer observations than typical)\n",
      "For trend analyses, we could exclude 2014, but we chose to keep it\n",
      "\n",
      "\n",
      "2.Observer Effort Distribution\n",
      "\n",
      "Concentration metrics:\n",
      "- Single top observer: 36.5% of all observations\n",
      "- Top 5 observers: 61.1% of all observations\n",
      "- Total observers: 42\n",
      "\n",
      "Interpretation:\n",
      "Heavily skewed observer effort (one person > 1/3 of data)\n",
      "Potential for observer-specific detection biases\n",
      "Should be acknowledged as limitation in temporal trends\n",
      "\n",
      "\n",
      "3. Species Metadata Completeness\n",
      "Species counts:\n",
      "Unique species in observations: 102\n",
      "Species in metadata lookup: 86\n",
      "Species WITHOUT metadata: 16\n",
      "\n",
      "Interpretation:\n",
      "Cannot obtain scientific names or migration status for missing species\n",
      "These species will be retained in abundance analyses\n",
      "Migration status analyses will exclude these species\n",
      "\n",
      "\n",
      "4. Detection Method Data Availability\n",
      "Detection method breakdown:\n",
      "  count_auditory                     :  99.8% non-null,  75.4% > 0\n",
      "  count_visual_no_flight             : 100.0% non-null,  38.9% > 0\n",
      "  count_audio_visual_no_flight       : 100.0% non-null,  98.7% > 0\n",
      "  count_audio_visual_flight          : 100.0% non-null, 100.0% > 0\n",
      "\n",
      "Interpretation:\n",
      "Detection method data is available (auditory/visual breakdown)\n",
      "Can analyze detection patterns if needed\n",
      "Total counts already aggregated across all detection methods\n",
      "\n",
      "\n",
      "5. Weather Data Quality\n",
      "Missing values in weather columns:\n",
      "  wind                :     22 missing (  0.0%)\n",
      "  rain                :      1 missing (  0.0%)\n",
      "  visibility          :      1 missing (  0.0%)\n",
      "  cloud_cover_raw     :      1 missing (  0.0%)\n",
      "\n",
      "Invalid values detected:\n",
      "  - Negative wind values: 5 (corrected to NaN)\n",
      "\n",
      "Interpretation:\n",
      "Weather data has some missing values but generally usable\n",
      "Data quality issues were corrected during cleaning\n",
      "\n",
      "\n",
      "6. Data Cleaning Summary\n",
      "Original records: 114,497\n",
      "Final records:    114,493\n",
      "Records removed:  4\n",
      "\n",
      "Cleaning actions taken:\n",
      "Set 5 negative wind values to NaN\n",
      "Removed 4 records with zero/negative counts\n",
      "Removed records missing essential fields (year/species/transect)\n",
      "Created observation IDs for tracking\n",
      "Extracted year from date column\n"
     ]
    }
   ],
   "source": [
    "# Data Quality Analysis\n",
    "print(\"Data Quality Analysis\")\n",
    "\n",
    "# Issue 1: Temporal Coverage Completeness\n",
    "print(\"\\n1. Temporal Coverage Analysis\")\n",
    "# Calculate observations per year\n",
    "obs_per_year = df_clean.groupby('year').size().sort_index()\n",
    "\n",
    "# Calculate mean and std for complete years (exclude first/last to be conservative)\n",
    "middle_years = obs_per_year.iloc[1:-1]\n",
    "mean_obs = middle_years.mean()\n",
    "std_obs = middle_years.std()\n",
    "threshold = mean_obs - 3 * std_obs  # Flag if more than 3 SD below mean\n",
    "\n",
    "print(f\"\\nStatistical threshold for completeness:\")\n",
    "print(f\"Mean observations (2015-2024): {mean_obs:,.0f}\")\n",
    "print(f\"Standard deviation: {std_obs:,.0f}\")\n",
    "print(f\"Threshold (mean - 3×SD): {threshold:,.0f}\")\n",
    "\n",
    "# Identify partial years\n",
    "partial_years = obs_per_year[obs_per_year < threshold].index.tolist()\n",
    "\n",
    "if partial_years:\n",
    "    print(f\"\\nPartial years identified: {', '.join(map(str, partial_years))}\")\n",
    "    for year in partial_years:\n",
    "        pct_of_mean = (obs_per_year[year] / mean_obs) * 100\n",
    "        print(f\"  {year}: {obs_per_year[year]:,} obs ({pct_of_mean:.0f}% of typical year)\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"2014 is partial (significantly fewer observations than typical)\")\n",
    "print(\"For trend analyses, we could exclude 2014, but we chose to keep it\")\n",
    "\n",
    "# Issue 2: Observer Effort Distribution\n",
    "print(\"\\n\\n2.Observer Effort Distribution\")\n",
    "obs_counts = df_clean.groupby('observer_name').size().sort_values(ascending=False)\n",
    "total_obs = len(df_clean)\n",
    "\n",
    "# Calculate concentration metrics\n",
    "top5_pct = (obs_counts.head(5).sum() / total_obs) * 100\n",
    "top1_pct = (obs_counts.iloc[0] / total_obs) * 100\n",
    "\n",
    "print(f\"\\nConcentration metrics:\")\n",
    "print(f\"- Single top observer: {top1_pct:.1f}% of all observations\")\n",
    "print(f\"- Top 5 observers: {top5_pct:.1f}% of all observations\")\n",
    "print(f\"- Total observers: {len(obs_counts)}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"Heavily skewed observer effort (one person > 1/3 of data)\")\n",
    "print(\"Potential for observer-specific detection biases\")\n",
    "print(\"Should be acknowledged as limitation in temporal trends\")\n",
    "\n",
    "# Issue 3: Species Metadata Completeness\n",
    "print(\"\\n\\n3. Species Metadata Completeness\")\n",
    "n_species_obs = df_clean['species_name'].nunique()\n",
    "n_species_lookup = len(df_species)\n",
    "n_missing = n_species_obs - n_species_lookup\n",
    "\n",
    "# Find which species are missing\n",
    "species_in_obs = set(df_clean['species_name'].unique())\n",
    "species_in_lookup = set(df_species['french_name'].unique())\n",
    "missing_species = species_in_obs - species_in_lookup\n",
    "\n",
    "print(f\"Species counts:\")\n",
    "print(f\"Unique species in observations: {n_species_obs}\")\n",
    "print(f\"Species in metadata lookup: {n_species_lookup}\")\n",
    "print(f\"Species WITHOUT metadata: {n_missing}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"Cannot obtain scientific names or migration status for missing species\")\n",
    "print(\"These species will be retained in abundance analyses\")\n",
    "print(\"Migration status analyses will exclude these species\")\n",
    "\n",
    "# Issue 4: Detection Method Data Availability\n",
    "print(\"\\n\\n4. Detection Method Data Availability\")\n",
    "\n",
    "# Check detection method columns\n",
    "detection_cols = ['count_auditory', 'count_visual_no_flight', \n",
    "                 'count_audio_visual_no_flight', 'count_audio_visual_flight']\n",
    "\n",
    "print(\"Detection method breakdown:\")\n",
    "detection_summary = []\n",
    "for col in detection_cols:\n",
    "    if col in df_clean.columns:\n",
    "        n_present = df_clean[col].notna().sum()\n",
    "        n_nonzero = (df_clean[col] > 0).sum()\n",
    "        pct_present = (n_present / len(df_clean)) * 100\n",
    "        pct_nonzero = (n_nonzero / len(df_clean)) * 100\n",
    "        print(f\"  {col:<35}: {pct_present:5.1f}% non-null, {pct_nonzero:5.1f}% > 0\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"Detection method data is available (auditory/visual breakdown)\")\n",
    "print(\"Can analyze detection patterns if needed\")\n",
    "print(\"Total counts already aggregated across all detection methods\")\n",
    "\n",
    "# Issue 5: Weather Data Quality\n",
    "print(\"\\n\\n5. Weather Data Quality\")\n",
    "weather_cols = ['wind', 'rain', 'visibility', 'cloud_cover_raw']\n",
    "print(\"Missing values in weather columns:\")\n",
    "for col in weather_cols:\n",
    "    if col in df_clean.columns:\n",
    "        n_missing = df_clean[col].isna().sum()\n",
    "        pct_missing = (n_missing / len(df_clean)) * 100\n",
    "        print(f\"  {col:<20}: {n_missing:6,} missing ({pct_missing:5.1f}%)\")\n",
    "\n",
    "# Check for invalid values\n",
    "print(\"\\nInvalid values detected:\")\n",
    "if 'wind' in df_clean.columns:\n",
    "    n_negative_wind = (df_obs['wind'] < 0).sum()  # Check original before cleaning\n",
    "    print(f\"  - Negative wind values: {n_negative_wind} (corrected to NaN)\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"Weather data has some missing values but generally usable\")\n",
    "print(\"Data quality issues were corrected during cleaning\")\n",
    "\n",
    "# Issue 6: Data Cleaning Summary\n",
    "print(\"\\n\\n6. Data Cleaning Summary\")\n",
    "\n",
    "print(f\"Original records: {len(df_obs):,}\")\n",
    "print(f\"Final records:    {len(df_clean):,}\")\n",
    "print(f\"Records removed:  {len(df_obs) - len(df_clean):,}\")\n",
    "\n",
    "print(\"\\nCleaning actions taken:\")\n",
    "print(f\"Set {(df_obs['wind'] < 0).sum()} negative wind values to NaN\")\n",
    "print(f\"Removed 4 records with zero/negative counts\")\n",
    "print(f\"Removed records missing essential fields (year/species/transect)\")\n",
    "print(f\"Created observation IDs for tracking\")\n",
    "print(f\"Extracted year from date column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977da45c",
   "metadata": {},
   "source": [
    "### Detection Modality Data\n",
    "\n",
    "The dataset contains separate count columns for different detection methods:\n",
    "- `count_auditory`: Birds detected by sound only\n",
    "- `count_visual_no_flight`: Birds detected visually (not flying)  \n",
    "- `count_audio_visual_no_flight`: Birds detected by both sound and sight (not flying)\n",
    "- `count_audio_visual_flight`: Birds detected by both methods while in flight\n",
    "\n",
    "These are summed to create `individual_count`. While the original dataset uses multiple columns tracking detection modality, we chose to not focus on this. Distance sampling data (Unnamed: 13-20) remains largely empty (>70% missing cells) and will not be used in our trend analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d33e4897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_observations</th>\n",
       "      <th>n_species</th>\n",
       "      <th>total_abundance</th>\n",
       "      <th>n_transects</th>\n",
       "      <th>n_observers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>5377</td>\n",
       "      <td>63</td>\n",
       "      <td>32643.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>8162</td>\n",
       "      <td>70</td>\n",
       "      <td>52453.000000</td>\n",
       "      <td>53</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>11010</td>\n",
       "      <td>77</td>\n",
       "      <td>69062.000000</td>\n",
       "      <td>59</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>9228</td>\n",
       "      <td>71</td>\n",
       "      <td>61685.380952</td>\n",
       "      <td>60</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>9733</td>\n",
       "      <td>73</td>\n",
       "      <td>61208.000000</td>\n",
       "      <td>65</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>9572</td>\n",
       "      <td>70</td>\n",
       "      <td>63217.000000</td>\n",
       "      <td>64</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>9956</td>\n",
       "      <td>68</td>\n",
       "      <td>66171.000000</td>\n",
       "      <td>63</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>10604</td>\n",
       "      <td>69</td>\n",
       "      <td>72048.000000</td>\n",
       "      <td>63</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>10804</td>\n",
       "      <td>68</td>\n",
       "      <td>73549.000000</td>\n",
       "      <td>63</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>10224</td>\n",
       "      <td>67</td>\n",
       "      <td>67057.000000</td>\n",
       "      <td>64</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>10051</td>\n",
       "      <td>70</td>\n",
       "      <td>65395.000000</td>\n",
       "      <td>65</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>9772</td>\n",
       "      <td>72</td>\n",
       "      <td>63151.000000</td>\n",
       "      <td>65</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      n_observations  n_species  total_abundance  n_transects  n_observers\n",
       "year                                                                      \n",
       "2014            5377         63     32643.000000           41           24\n",
       "2015            8162         70     52453.000000           53           26\n",
       "2016           11010         77     69062.000000           59           28\n",
       "2017            9228         71     61685.380952           60           27\n",
       "2018            9733         73     61208.000000           65           23\n",
       "2019            9572         70     63217.000000           64           21\n",
       "2020            9956         68     66171.000000           63           15\n",
       "2021           10604         69     72048.000000           63           13\n",
       "2022           10804         68     73549.000000           63           15\n",
       "2023           10224         67     67057.000000           64           14\n",
       "2024           10051         70     65395.000000           65           24\n",
       "2025            9772         72     63151.000000           65           26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get annual summary\n",
    "annual_summary = get_annual_summary(df_clean)\n",
    "annual_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0939380c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Dataset Overview\n",
      "\n",
      "Temporal Coverage:\n",
      "  First observation: 2014-03-29\n",
      "  Last observation:  2025-07-03\n",
      "  Total years:       12\n",
      "  Year range:        2014 - 2025\n",
      "\n",
      "Spatial Coverage:\n",
      "  Unique transects:  72\n",
      "  Unique points:     10 (1-10 per transect)\n",
      "\n",
      "Species Diversity:\n",
      "  Unique species:    102\n",
      "  Total individuals: 747,639\n",
      "\n",
      "Sampling Effort:\n",
      "  Total observations: 114,493\n",
      "  Unique observers:   42\n",
      "\n",
      "Count Statistics:\n",
      "  Mean birds/observation:   6.5\n",
      "  Median birds/observation: 6\n",
      "  Max birds/observation:    600\n"
     ]
    }
   ],
   "source": [
    "# Basic Dataset Overview\n",
    "print(\"Basic Dataset Overview\")\n",
    "\n",
    "print(\"\\nTemporal Coverage:\")\n",
    "print(f\"  First observation: {df_clean['date'].min().strftime('%Y-%m-%d')}\")\n",
    "print(f\"  Last observation:  {df_clean['date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"  Total years:       {df_clean['year'].nunique()}\")\n",
    "print(f\"  Year range:        {int(df_clean['year'].min())} - {int(df_clean['year'].max())}\")\n",
    "\n",
    "print(\"\\nSpatial Coverage:\")\n",
    "print(f\"  Unique transects:  {df_clean['transect_name'].nunique()}\")\n",
    "print(f\"  Unique points:     {df_clean['point_number'].nunique()} (1-10 per transect)\")\n",
    "\n",
    "print(\"\\nSpecies Diversity:\")\n",
    "print(f\"  Unique species:    {df_clean['species_name'].nunique()}\")\n",
    "print(f\"  Total individuals: {df_clean['individual_count'].sum():,.0f}\")\n",
    "\n",
    "print(\"\\nSampling Effort:\")\n",
    "print(f\"  Total observations: {len(df_clean):,}\")\n",
    "print(f\"  Unique observers:   {df_clean['observer_name'].nunique()}\")\n",
    "\n",
    "print(\"\\nCount Statistics:\")\n",
    "print(f\"  Mean birds/observation:   {df_clean['individual_count'].mean():.1f}\")\n",
    "print(f\"  Median birds/observation: {df_clean['individual_count'].median():.0f}\")\n",
    "print(f\"  Max birds/observation:    {df_clean['individual_count'].max():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c9169",
   "metadata": {},
   "source": [
    "## Data Quality Assessment\n",
    "\n",
    "The following sections examine data quality issues that may affect downstream analyses.\n",
    "\n",
    "### Weather Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac0ead53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather Conditions During Surveys\n",
      "   Variable  Non-Missing Missing (%)\n",
      "       Wind       114471        0.0%\n",
      "       Rain       114492        0.0%\n",
      "Cloud Cover       114492        0.0%\n",
      " Visibility       114492        0.0%\n",
      "\n",
      "Weather Variable Distributions:\n",
      "                wind           rain    cloud_cover     visibility\n",
      "count  114471.000000  114492.000000  114491.000000  114492.000000\n",
      "mean        1.423068       1.029338       1.968163       1.419549\n",
      "std         0.600080       0.177482       0.698644       0.619842\n",
      "min         0.000000       1.000000       1.000000       1.000000\n",
      "25%         1.000000       1.000000       1.000000       1.000000\n",
      "50%         1.000000       1.000000       2.000000       1.000000\n",
      "75%         2.000000       1.000000       2.000000       2.000000\n",
      "max         3.000000       3.000000       3.000000       4.000000\n"
     ]
    }
   ],
   "source": [
    "# Environmental Conditions Summary\n",
    "print(\"Weather Conditions During Surveys\")\n",
    "\n",
    "weather_summary = pd.DataFrame({\n",
    "    'Variable': ['Wind', 'Rain', 'Cloud Cover', 'Visibility'],\n",
    "    'Non-Missing': [\n",
    "        df_clean['wind'].notna().sum(),\n",
    "        df_clean['rain'].notna().sum(),\n",
    "        df_clean['cloud_cover_raw'].notna().sum(),\n",
    "        df_clean['visibility'].notna().sum()\n",
    "    ],\n",
    "    'Missing (%)': [\n",
    "        f\"{df_clean['wind'].isna().sum() / len(df_clean) * 100:.1f}%\",\n",
    "        f\"{df_clean['rain'].isna().sum() / len(df_clean) * 100:.1f}%\",\n",
    "        f\"{df_clean['cloud_cover_raw'].isna().sum() / len(df_clean) * 100:.1f}%\",\n",
    "        f\"{df_clean['visibility'].isna().sum() / len(df_clean) * 100:.1f}%\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(weather_summary.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Convert cloud_cover to numeric for descriptive stats\n",
    "df_clean['cloud_cover'] = pd.to_numeric(df_clean['cloud_cover_raw'], errors='coerce')\n",
    "\n",
    "# Descriptive statistics for weather variables\n",
    "print(\"Weather Variable Distributions:\")\n",
    "print(df_clean[['wind', 'rain', 'cloud_cover', 'visibility']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1236fa6",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "We assess completeness across all columns to identify data gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5dabddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Analysis\n",
      "\n",
      "Columns with missing values:\n",
      "                      Column  Missing_Count  Missing_Percent\n",
      "                 Unnamed: 19         113105             98.8\n",
      "                 Unnamed: 17         112962             98.7\n",
      "                 Unnamed: 20         112672             98.4\n",
      "                 Unnamed: 18         111845             97.7\n",
      "                 Unnamed: 15         109061             95.3\n",
      "                 Unnamed: 16         106091             92.7\n",
      "                 Unnamed: 14          83345             72.8\n",
      "                 Unnamed: 13          75302             65.8\n",
      "       distance_category_raw          55664             48.6\n",
      "              count_auditory            240              0.2\n",
      "count_audio_visual_no_flight              6              0.0\n",
      "      count_visual_no_flight              4              0.0\n",
      "             department_code              2              0.0\n",
      "                  visibility              1              0.0\n",
      "                        wind             22              0.0\n",
      "                        rain              1              0.0\n",
      "             cloud_cover_raw              1              0.0\n",
      "                 cloud_cover              2              0.0\n",
      "\n",
      "18 out of 30 columns have missing data\n",
      "\n",
      "Essential columns: 8/8 complete\n",
      "  All core fields required for trend analysis are complete ✓\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Missing Values Analysis\n",
    "print(\"Missing Values Analysis\\n\")\n",
    "\n",
    "# Create full missing values table\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Column': df_clean.columns,\n",
    "    'Missing_Count': df_clean.isnull().sum(),\n",
    "    'Missing_Percent': (df_clean.isnull().sum() / len(df_clean) * 100).round(1)\n",
    "}).sort_values('Missing_Percent', ascending=False)\n",
    "\n",
    "# Filter to columns with missing data\n",
    "missing_with_data = missing_analysis[missing_analysis['Missing_Count'] > 0]\n",
    "\n",
    "print(\"Columns with missing values:\")\n",
    "print(missing_with_data.to_string(index=False))\n",
    "\n",
    "print(f\"\\n{len(missing_with_data)} out of {len(df_clean.columns)} columns have missing data\")\n",
    "\n",
    "# Highlight essential columns status (we dont use the detection modality columns)\n",
    "essential_cols = ['observer_name', 'transect_name', 'species_name', 'date', \n",
    "                  'year', 'individual_count', 'point_number', 'visit_number']\n",
    "essential_complete = [col for col in essential_cols if df_clean[col].isnull().sum() == 0]\n",
    "\n",
    "print(f\"\\nEssential columns: {len(essential_complete)}/{len(essential_cols)} complete\")\n",
    "print(\"  All core fields required for trend analysis are complete ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3470abae",
   "metadata": {},
   "source": [
    "### Outlier Detection\n",
    "\n",
    "We use the IQR method to identify unusually high count observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55db7034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier Analysis\n",
      "\n",
      "Count Distribution:\n",
      "  Q1 (25th percentile): 3.0\n",
      "  Q3 (75th percentile): 9.0\n",
      "  IQR: 6.0\n",
      "  Upper outlier threshold: 18.0\n",
      "\n",
      "Outliers detected: 3364 observations (2.94%)\n",
      "  Max count: 600 individuals\n",
      "\n",
      "Top 5 largest counts:\n",
      "      date     transect_name       species_name  individual_count                                  observer_name\n",
      "2025-06-18 Ilets du François  Sterne de Dougall             600.0 AGAT Arnaud-FREMACX Vincent- NOSEL Christopher\n",
      "2025-05-06   Ilets du Robert Sterne fuligineuse             600.0                              BELLUMEUR Wilfrid\n",
      "2018-05-25    Fonds Préville   Héron garde-bœuf             456.0                             MOURIESSE Jocelyne\n",
      "2020-06-10   Ilets du Robert Sterne fuligineuse             450.0                              BELLUMEUR Wilfrid\n",
      "2022-06-10   Ilets du Robert Sterne fuligineuse             450.0                              BELLUMEUR Wilfrid\n"
     ]
    }
   ],
   "source": [
    "# Outlier Analysis for Count Data\n",
    "print(\"Outlier Analysis\\n\")\n",
    "\n",
    "# Calculate outlier thresholds using IQR method\n",
    "Q1 = df_clean['individual_count'].quantile(0.25)\n",
    "Q3 = df_clean['individual_count'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df_clean[df_clean['individual_count'] > upper_bound]\n",
    "\n",
    "print(f\"Count Distribution:\")\n",
    "print(f\"  Q1 (25th percentile): {Q1:.1f}\")\n",
    "print(f\"  Q3 (75th percentile): {Q3:.1f}\")\n",
    "print(f\"  IQR: {IQR:.1f}\")\n",
    "print(f\"  Upper outlier threshold: {upper_bound:.1f}\")\n",
    "print(f\"\\nOutliers detected: {len(outliers)} observations ({len(outliers)/len(df_clean)*100:.2f}%)\")\n",
    "print(f\"  Max count: {df_clean['individual_count'].max():.0f} individuals\")\n",
    "\n",
    "# Show a few examples of outliers\n",
    "if len(outliers) > 0:\n",
    "    print(f\"\\nTop 5 largest counts:\")\n",
    "    print(outliers.nlargest(5, 'individual_count')[\n",
    "        ['date', 'transect_name', 'species_name', 'individual_count', 'observer_name']\n",
    "    ].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af69ecca",
   "metadata": {},
   "source": [
    "## Exploratory Visualizations\n",
    "\n",
    "The following plots examine temporal patterns, spatial coverage, species diversity, and sampling effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a697d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Coverage Plot\n",
    "observations_per_year = df_clean.groupby('year').size()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.bar(observations_per_year.index.astype(int), observations_per_year.values, \n",
    "       color=COLORS['primary'], edgecolor='black', alpha=0.8)\n",
    "\n",
    "# Highlight partial years\n",
    "partial_years = [2014]\n",
    "for year in partial_years:\n",
    "    if year in observations_per_year.index:\n",
    "        ax.bar(year, observations_per_year[year], \n",
    "               color=COLORS['warning'], edgecolor='black', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Year', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Number of Observations', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Temporal Coverage: Observation Effort Over Time', \n",
    "             fontweight='bold', fontsize=14)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add text annotation for partial years\n",
    "ax.text(2014, observations_per_year[2014] * 1.05, 'Partial', \n",
    "        ha='center', fontsize=9, color='darkred', fontweight='bold')\n",
    "ax.text(2025, observations_per_year[2025] * 1.05, 'Partial', \n",
    "        ha='center', fontsize=9, color='darkred', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save with robust method\n",
    "os.makedirs('../figures', exist_ok=True)\n",
    "fig.savefig('../figures/eda_temporal_coverage.png', dpi=300, bbox_inches='tight', format='png')\n",
    "print(\"Saved: ../figures/eda_temporal_coverage.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41170ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial Coverage Plot\n",
    "transects_per_year = df_clean.groupby('year')['transect_name'].nunique()\n",
    "total_transects = df_clean['transect_name'].nunique()\n",
    "coverage_pct = (transects_per_year / total_transects * 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.bar(transects_per_year.index.astype(int), coverage_pct.values, \n",
    "       color=COLORS['accent'], edgecolor='black', alpha=0.8)\n",
    "ax.axhline(y=100, color=COLORS['warning'], linestyle='--', \n",
    "           linewidth=2, label='Full coverage')\n",
    "ax.set_xlabel('Year', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Spatial Coverage (%)', fontweight='bold', fontsize=12)\n",
    "ax.set_title(f'Spatial Coverage by Year ({total_transects} transects total)', \n",
    "             fontweight='bold', fontsize=14)\n",
    "ax.legend(frameon=True, shadow=True)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save with robust method\n",
    "os.makedirs('../figures', exist_ok=True)\n",
    "fig.savefig('../figures/eda_spatial_coverage.png', dpi=300, bbox_inches='tight', format='png')\n",
    "print(\"Saved: ../figures/eda_spatial_coverage.png\")\n",
    "# Plot will display automatically in notebook with %matplotlib inline\n",
    "\n",
    "# Sampling Effort Variation Analysis\n",
    "print(\"\\n Sampling Effort Variation\\n\")\n",
    "\n",
    "# Temporal variation\n",
    "effort_by_year = df_clean.groupby('year').size()\n",
    "cv_temporal = (effort_by_year.std() / effort_by_year.mean()) * 100\n",
    "\n",
    "print(f\"Temporal variation (coefficient of variation): {cv_temporal:.1f}%\")\n",
    "print(f\"  Min effort year: {effort_by_year.min():,} obs ({effort_by_year.idxmin()})\")\n",
    "print(f\"  Max effort year: {effort_by_year.max():,} obs ({effort_by_year.idxmax()})\")\n",
    "\n",
    "# Spatial variation\n",
    "effort_by_transect = df_clean.groupby('transect_name').size()\n",
    "cv_spatial = (effort_by_transect.std() / effort_by_transect.mean()) * 100\n",
    "\n",
    "print(f\"\\nSpatial variation (coefficient of variation): {cv_spatial:.1f}%\")\n",
    "print(f\"  Min effort transect: {effort_by_transect.min()} obs\")\n",
    "print(f\"  Max effort transect: {effort_by_transect.max()} obs\")\n",
    "\n",
    "print(\"\\nImplication: Effort variation means we should\")\n",
    "print(\"  - Use effort-standardized metrics (counts per point)\")\n",
    "print(\"  - Account for sampling intensity in temporal trend models\")\n",
    "print(\"  - Consider rarefaction for species richness comparisons\")\n",
    "\n",
    "print(\"\\nKey insight: No year achieved 100% transect coverage\")\n",
    "print(f\"  Average coverage: {coverage_pct.mean():.0f}%\")\n",
    "print(\"  This variation must be considered in temporal trend analyses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf16ab06",
   "metadata": {},
   "source": [
    "### Species: simple summaries and plots\n",
    "\n",
    "Below we summarise species-level totals and richness over time to understand which species dominate counts and how diversity changes annually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0121a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 species by total individuals\n",
    "species_totals = (\n",
    "    df_clean.groupby('species_name')['individual_count']\n",
    "            .sum()\n",
    "            .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "top_n = 20\n",
    "top_species = species_totals.head(top_n)\n",
    "\n",
    "# Create figure and plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot horizontal bars\n",
    "ax.barh(top_species.index[::-1], top_species.values[::-1], \n",
    "        color=COLORS['success'], edgecolor='black', alpha=0.8)\n",
    "\n",
    "# Styling\n",
    "ax.set_xlabel('Total individuals (2014-2025)', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Species', fontweight='bold', fontsize=12)\n",
    "ax.set_title(f'Top {top_n} Species by Abundance', fontweight='bold', fontsize=14)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "filename = '../figures/eda_species_top20.png'\n",
    "parent = os.path.dirname(filename)\n",
    "if parent:\n",
    "    os.makedirs(parent, exist_ok=True)\n",
    "fig.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b8dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative species accumulation\n",
    "cumulative_species = []\n",
    "all_species_seen = set()\n",
    "\n",
    "for year in sorted(df_clean['year'].unique()):\n",
    "    year_species = set(df_clean[df_clean['year'] == year]['species_name'])\n",
    "    all_species_seen.update(year_species)\n",
    "    cumulative_species.append(len(all_species_seen))\n",
    "\n",
    "years_sorted = sorted(df_clean['year'].unique())\n",
    "\n",
    "# Create figure and plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot line\n",
    "ax.plot(years_sorted, cumulative_species, marker='o', linewidth=2, \n",
    "        color=COLORS['success'], markersize=8)\n",
    "\n",
    "# Highlight specific points\n",
    "highlight_points = [2014, 2025]\n",
    "for point in highlight_points:\n",
    "    if point in years_sorted:\n",
    "        idx = list(years_sorted).index(point)\n",
    "        ax.scatter([point], [cumulative_species[idx]], color=COLORS['warning'], \n",
    "                  s=100, zorder=3, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add labels\n",
    "for point, label in zip(highlight_points, ['Partial', 'Partial']):\n",
    "    if point in years_sorted:\n",
    "        idx = list(years_sorted).index(point)\n",
    "        ax.text(point, cumulative_species[idx] + max(cumulative_species)*0.03, label, \n",
    "               ha='center', fontsize=9, color='darkred', fontweight='bold')\n",
    "\n",
    "# Styling\n",
    "ax.set_xlabel('Year', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Cumulative unique species', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Species Accumulation Over Time', fontweight='bold', fontsize=14)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "filename = '../figures/eda_species_accumulation.png'\n",
    "parent = os.path.dirname(filename)\n",
    "if parent:\n",
    "    os.makedirs(parent, exist_ok=True)\n",
    "fig.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf401e4",
   "metadata": {},
   "source": [
    "### Observer effort: simple summaries and plots\n",
    "\n",
    "We assess how sampling effort is distributed across observers and over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87d44de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observer Effort Analysis\n",
    "obs_by_observer = df_clean.groupby('observer_name').size().sort_values(ascending=False)\n",
    "transects_by_observer = df_clean.groupby('observer_name')['transect_name'].nunique().sort_values(ascending=False)\n",
    "\n",
    "n = 10\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left plot\n",
    "ax1.barh(obs_by_observer.head(n).index[::-1], \n",
    "         obs_by_observer.head(n).values[::-1], \n",
    "         color=COLORS['primary'], edgecolor='black', alpha=0.8)\n",
    "ax1.set_xlabel('Number of observations', fontweight='bold', fontsize=12)\n",
    "ax1.set_title(f'Top {n} Observers by Observation Count', fontweight='bold', fontsize=14)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Right plot\n",
    "ax2.barh(transects_by_observer.head(n).index[::-1], \n",
    "         transects_by_observer.head(n).values[::-1], \n",
    "         color=COLORS['accent'], edgecolor='black', alpha=0.8)\n",
    "ax2.set_xlabel('Unique transects', fontweight='bold', fontsize=12)\n",
    "ax2.set_title(f'Top {n} Observers by Spatial Coverage', fontweight='bold', fontsize=14)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "filename = '../figures/eda_observer_effort.png'\n",
    "parent = os.path.dirname(filename)\n",
    "if parent:\n",
    "    os.makedirs(parent, exist_ok=True)\n",
    "fig.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
