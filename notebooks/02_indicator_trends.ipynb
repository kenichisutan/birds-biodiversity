{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eeabfbc",
   "metadata": {},
   "source": [
    "# ðŸ“¦ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a92935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from data_io import load_excel_data, clean_observations, get_annual_summary\n",
    "\n",
    "# set some plotting defaults\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a6f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = load_excel_data('../data/raw/Observations 2012-2025.xlsx')\n",
    "df_obs = data['observations']\n",
    "df_species = data['species']\n",
    "df_gps = data['gps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48abf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. LOAD & PREPARE DATA\n",
    "# ============================================================\n",
    "\n",
    "# Import additional libraries needed for analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Clean the observations data\n",
    "df_clean = clean_observations(df_obs)\n",
    "\n",
    "# Filter data for years 2015-2024\n",
    "df_filtered = df_clean[(df_clean['year'] >= 2015) & (df_clean['year'] <= 2024)].copy()\n",
    "\n",
    "print(f\"Filtered data for years 2015-2024: {len(df_filtered)} observations\")\n",
    "print(f\"Year range: {df_filtered['year'].min()} - {df_filtered['year'].max()}\")\n",
    "print(f\"Unique species: {df_filtered['species_name'].nunique()}\")\n",
    "print(f\"Unique years: {sorted(df_filtered['year'].unique())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca7de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. COMPUTE DIVERSITY METRICS PER YEAR\n",
    "# ============================================================\n",
    "\n",
    "def calculate_shannon_diversity(df_year):\n",
    "    \"\"\"\n",
    "    Calculate Shannon diversity index for a given year's data.\n",
    "    H' = -Î£(p_i * ln(p_i)), where p_i = proportion of individuals per species\n",
    "    \"\"\"\n",
    "    # Get total count per species for this year\n",
    "    species_counts = df_year.groupby('species_name')['individual_count'].sum()\n",
    "    \n",
    "    # Calculate total individuals\n",
    "    total_individuals = species_counts.sum()\n",
    "    \n",
    "    if total_individuals == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # Calculate proportions\n",
    "    proportions = species_counts / total_individuals\n",
    "    \n",
    "    # Remove zero proportions (they don't contribute to Shannon)\n",
    "    proportions = proportions[proportions > 0]\n",
    "    \n",
    "    # Calculate Shannon index\n",
    "    shannon = -np.sum(proportions * np.log(proportions))\n",
    "    \n",
    "    return shannon\n",
    "\n",
    "# Compute metrics for each year\n",
    "annual_metrics = []\n",
    "\n",
    "for year in sorted(df_filtered['year'].unique()):\n",
    "    df_year = df_filtered[df_filtered['year'] == year]\n",
    "    \n",
    "    # Species richness (number of unique species)\n",
    "    richness = df_year['species_name'].nunique()\n",
    "    \n",
    "    # Shannon diversity\n",
    "    shannon = calculate_shannon_diversity(df_year)\n",
    "    \n",
    "    # Number of records\n",
    "    n_records = len(df_year)\n",
    "    \n",
    "    annual_metrics.append({\n",
    "        'year': year,\n",
    "        'richness': richness,\n",
    "        'shannon': shannon,\n",
    "        'n_records': n_records\n",
    "    })\n",
    "\n",
    "# Create dataframe\n",
    "df_annual = pd.DataFrame(annual_metrics)\n",
    "df_annual = df_annual.sort_values('year').reset_index(drop=True)\n",
    "\n",
    "print(\"Annual Diversity Metrics (2015-2024):\")\n",
    "print(df_annual.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. BOOTSTRAP CONFIDENCE INTERVALS (1000 ITERATIONS)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "n_bootstrap = 1000\n",
    "\n",
    "bootstrap_results = []\n",
    "\n",
    "print(\"Computing bootstrap confidence intervals (1000 iterations)...\")\n",
    "\n",
    "for year in sorted(df_filtered['year'].unique()):\n",
    "    df_year = df_filtered[df_filtered['year'] == year]\n",
    "    n_obs = len(df_year)\n",
    "    \n",
    "    # Preallocate lists for efficiency\n",
    "    richness_bootstrap = np.empty(n_bootstrap)\n",
    "    shannon_bootstrap = np.empty(n_bootstrap)\n",
    "    shannon_bootstrap[:] = np.nan  # Initialize with NaN\n",
    "\n",
    "    # Bootstrap iterations\n",
    "    for i in range(n_bootstrap):\n",
    "        # Resample with replacement\n",
    "        df_boot = df_year.sample(n=n_obs, replace=True)\n",
    "        \n",
    "        # Richness: number of unique species\n",
    "        richness_bootstrap[i] = df_boot['species_name'].nunique()\n",
    "        \n",
    "        # Shannon diversity (if defined)\n",
    "        shannon_val = calculate_shannon_diversity(df_boot)\n",
    "        if not np.isnan(shannon_val):\n",
    "            shannon_bootstrap[i] = shannon_val\n",
    "    \n",
    "    # ===== Compute Richness Confidence Intervals =====\n",
    "    richness_ci_lower, richness_ci_upper = np.percentile(richness_bootstrap, [2.5, 97.5])\n",
    "    \n",
    "    # ===== Compute Shannon Confidence Intervals =====\n",
    "    valid_shannon = shannon_bootstrap[~np.isnan(shannon_bootstrap)]\n",
    "    if valid_shannon.size > 0:\n",
    "        shannon_ci_lower, shannon_ci_upper = np.percentile(valid_shannon, [2.5, 97.5])\n",
    "    else:\n",
    "        shannon_ci_lower, shannon_ci_upper = np.nan, np.nan\n",
    "\n",
    "    bootstrap_results.append({\n",
    "        'year': year,\n",
    "        'richness_ci_lower': richness_ci_lower,\n",
    "        'richness_ci_upper': richness_ci_upper,\n",
    "        'shannon_ci_lower': shannon_ci_lower,\n",
    "        'shannon_ci_upper': shannon_ci_upper\n",
    "    })\n",
    "    \n",
    "    print(f\"  Year {year}: completed\")\n",
    "\n",
    "# Merge bootstrap results with annual metrics\n",
    "df_bootstrap = pd.DataFrame(bootstrap_results)\n",
    "df_annual = df_annual.merge(df_bootstrap, on='year', how='left')\n",
    "\n",
    "print(\"\\nBootstrap confidence intervals added:\")\n",
    "print(df_annual[['year', 'richness', 'richness_ci_lower', 'richness_ci_upper', \n",
    "                 'shannon', 'shannon_ci_lower', 'shannon_ci_upper']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff12c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. CHECK MODEL ASSUMPTIONS\n",
    "# ============================================================\n",
    "\n",
    "# Shapiro-Wilk test for normality of residuals\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL ASSUMPTION CHECKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Richness residuals\n",
    "stat_richness, p_richness = shapiro(model_richness.resid)\n",
    "print(\"\\nSpecies Richness Model:\")\n",
    "print(f\"  Shapiro-Wilk test statistic: {stat_richness:.4f}\")\n",
    "print(f\"  P-value (normality): {p_richness:.4f}\")\n",
    "print(f\"  Interpretation: {'Residuals appear normal' if p_richness > 0.05 else 'Residuals may not be normal'}\")\n",
    "\n",
    "# Shannon residuals\n",
    "stat_shannon, p_shannon = shapiro(model_shannon.resid)\n",
    "print(\"\\nShannon Diversity Model:\")\n",
    "print(f\"  Shapiro-Wilk test statistic: {stat_shannon:.4f}\")\n",
    "print(f\"  P-value (normality): {p_shannon:.4f}\")\n",
    "print(f\"  Interpretation: {'Residuals appear normal' if p_shannon > 0.05 else 'Residuals may not be normal'}\")\n",
    "\n",
    "# Create diagnostic plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Richness: Residuals vs Fitted\n",
    "axes[0, 0].scatter(model_richness.fittedvalues, model_richness.resid, alpha=0.6)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 0].set_xlabel('Fitted Values', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Residuals', fontweight='bold')\n",
    "axes[0, 0].set_title('Species Richness: Residuals vs Fitted', fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Richness: Q-Q plot\n",
    "stats.probplot(model_richness.resid, dist=\"norm\", plot=axes[0, 1])\n",
    "axes[0, 1].set_title('Species Richness: Q-Q Plot', fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Shannon: Residuals vs Fitted\n",
    "axes[1, 0].scatter(model_shannon.fittedvalues, model_shannon.resid, alpha=0.6)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Fitted Values', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Residuals', fontweight='bold')\n",
    "axes[1, 0].set_title('Shannon Diversity: Residuals vs Fitted', fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Shannon: Q-Q plot\n",
    "stats.probplot(model_shannon.resid, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Shannon Diversity: Q-Q Plot', fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/indicator_model_diagnostics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nSaved diagnostic plots: ../figures/indicator_model_diagnostics.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664973d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. CREATE PUBLICATION-QUALITY PLOTS\n",
    "# ============================================================\n",
    "\n",
    "# Generate prediction intervals for trend lines\n",
    "# Create a fine grid of years for smooth trend lines\n",
    "years_smooth = np.linspace(df_annual['year'].min(), df_annual['year'].max(), 100)\n",
    "years_smooth_centered = years_smooth - year_mean\n",
    "\n",
    "# Get predictions for trend lines\n",
    "X_smooth_richness = sm.add_constant(years_smooth_centered)\n",
    "pred_richness = model_richness.get_prediction(X_smooth_richness)\n",
    "pred_richness_frame = pred_richness.summary_frame(alpha=0.05)\n",
    "\n",
    "X_smooth_shannon = sm.add_constant(years_smooth_centered)\n",
    "pred_shannon = model_shannon.get_prediction(X_smooth_shannon)\n",
    "pred_shannon_frame = pred_shannon.summary_frame(alpha=0.05)\n",
    "\n",
    "# ============================================================\n",
    "# Plot 1: Annual Species Richness\n",
    "# ============================================================\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Plot observed data with error bars\n",
    "yerr_lower = (df_annual['richness'] - df_annual['richness_ci_lower']).clip(lower=0)\n",
    "yerr_upper = (df_annual['richness_ci_upper'] - df_annual['richness']).clip(lower=0)\n",
    "\n",
    "ax.errorbar(\n",
    "    df_annual['year'], df_annual['richness'],\n",
    "    yerr=[yerr_lower, yerr_upper],\n",
    "    fmt='o', color='#2E86AB', markersize=8, capsize=5, capthick=2,\n",
    "    label='Observed (95% CI)', zorder=3\n",
    ")\n",
    "\n",
    "# Plot trend line\n",
    "ax.plot(years_smooth, pred_richness_frame['mean'], \n",
    "        '--', color='#A23B72', linewidth=2.5, label='Linear trend', zorder=2)\n",
    "\n",
    "# Plot 95% prediction interval\n",
    "ax.fill_between(years_smooth, \n",
    "                pred_richness_frame['obs_ci_lower'], \n",
    "                pred_richness_frame['obs_ci_upper'],\n",
    "                alpha=0.2, color='#A23B72', label='95% Prediction interval', zorder=1)\n",
    "\n",
    "ax.set_xlabel('Year', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('Species Richness', fontweight='bold', fontsize=14)\n",
    "ax.set_title('Annual Species Richness (2015-2024)', fontweight='bold', fontsize=16)\n",
    "ax.grid(alpha=0.3, linestyle='--')\n",
    "ax.legend(loc='best', fontsize=11)\n",
    "\n",
    "# Add slope and significance annotation\n",
    "slope = model_richness.params['year_centered']\n",
    "pval = model_richness.pvalues['year_centered']\n",
    "significance = '***' if pval < 0.001 else '**' if pval < 0.01 else '*' if pval < 0.05 else 'ns'\n",
    "ax.text(0.02, 0.98, f'Slope: {slope:.3f} species/year {significance}\\nRÂ² = {model_richness.rsquared:.3f}',\n",
    "        transform=ax.transAxes, fontsize=11, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/indicator_species_richness.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: ../figures/indicator_species_richness.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350b2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Plot 2: Annual Shannon Diversity\n",
    "# ============================================================\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Plot observed data with error bars\n",
    "ax.errorbar(df_annual['year'], df_annual['shannon'], \n",
    "            yerr=[df_annual['shannon'] - df_annual['shannon_ci_lower'],\n",
    "                  df_annual['shannon_ci_upper'] - df_annual['shannon']],\n",
    "            fmt='o', color='#F18F01', markersize=8, capsize=5, capthick=2,\n",
    "            label='Observed (95% CI)', zorder=3)\n",
    "\n",
    "# Plot trend line\n",
    "ax.plot(years_smooth, pred_shannon_frame['mean'], \n",
    "        '--', color='#C73E1D', linewidth=2.5, label='Linear trend', zorder=2)\n",
    "\n",
    "# Plot 95% prediction interval\n",
    "ax.fill_between(years_smooth, \n",
    "                pred_shannon_frame['obs_ci_lower'], \n",
    "                pred_shannon_frame['obs_ci_upper'],\n",
    "                alpha=0.2, color='#C73E1D', label='95% Prediction interval', zorder=1)\n",
    "\n",
    "ax.set_xlabel('Year', fontweight='bold', fontsize=14)\n",
    "ax.set_ylabel('Shannon Diversity Index (H\\')', fontweight='bold', fontsize=14)\n",
    "ax.set_title('Annual Shannon Diversity Index (2015-2024)', fontweight='bold', fontsize=16)\n",
    "ax.grid(alpha=0.3, linestyle='--')\n",
    "ax.legend(loc='best', fontsize=11)\n",
    "\n",
    "# Add slope and significance annotation\n",
    "slope = model_shannon.params['year_centered']\n",
    "pval = model_shannon.pvalues['year_centered']\n",
    "significance = '***' if pval < 0.001 else '**' if pval < 0.01 else '*' if pval < 0.05 else 'ns'\n",
    "ax.text(0.02, 0.98, f'Slope: {slope:.4f} units/year {significance}\\nRÂ² = {model_shannon.rsquared:.3f}',\n",
    "        transform=ax.transAxes, fontsize=11, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/indicator_shannon_diversity.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: ../figures/indicator_shannon_diversity.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c01b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Person B - Abundance Metrics Analysis (Self-Contained Version)\n",
    "Birds Biodiversity Project - Section 2: Multi-Year Indicator Trends\n",
    "\n",
    "Indicators analyzed:\n",
    "1. Total Abundance - Raw population metric across all species\n",
    "2. Mean Abundance per Point - Effort-standardized abundance metric\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# STANDARD IMPORTS\n",
    "# ============================================================================\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import linregress\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ“ Standard imports successful\")\n",
    "\n",
    "from data_io import load_excel_data, clean_observations, get_annual_summary\n",
    "# ============================================================================\n",
    "# INDICATOR COMPUTATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_total_abundance(df):\n",
    "    \"\"\"\n",
    "    Compute total abundance (sum of all individual counts).\n",
    "    \"\"\"\n",
    "    return df['individual_count'].sum()\n",
    "\n",
    "def compute_mean_abundance_per_point(df):\n",
    "    \"\"\"\n",
    "    Compute mean abundance per observation point.\n",
    "    Standardizes for sampling effort by dividing total abundance\n",
    "    by the number of unique observation points visited.\n",
    "    \"\"\"\n",
    "    # Count unique observation points\n",
    "    n_points = df[['transect_name', 'point_number']].drop_duplicates().shape[0]\n",
    "    \n",
    "    # Total abundance\n",
    "    total_abundance = df['individual_count'].sum()\n",
    "    \n",
    "    # Mean per point\n",
    "    if n_points > 0:\n",
    "        return total_abundance / n_points\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "print(\"âœ“ Indicator functions defined\")\n",
    "\n",
    "# ============================================================================\n",
    "# STATISTICAL UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def bootstrap_ci(data, statistic_func, n_boot=1000, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Compute bootstrap confidence interval for a statistic.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Data to bootstrap\n",
    "    statistic_func : callable\n",
    "        Function that computes the statistic from data\n",
    "    n_boot : int\n",
    "        Number of bootstrap iterations\n",
    "    confidence_level : float\n",
    "        Confidence level (e.g., 0.95 for 95% CI)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with 'estimate', 'lower', 'upper', 'stderr'\n",
    "    \"\"\"\n",
    "    # Original estimate\n",
    "    estimate = statistic_func(data)\n",
    "    \n",
    "    # Bootstrap resampling\n",
    "    boot_stats = []\n",
    "    n = len(data)\n",
    "    \n",
    "    for _ in range(n_boot):\n",
    "        # Resample with replacement\n",
    "        sample = data.sample(n=n, replace=True)\n",
    "        boot_stats.append(statistic_func(sample))\n",
    "    \n",
    "    boot_stats = np.array(boot_stats)\n",
    "    \n",
    "    # Compute confidence interval\n",
    "    alpha = 1 - confidence_level\n",
    "    lower = np.percentile(boot_stats, 100 * alpha / 2)\n",
    "    upper = np.percentile(boot_stats, 100 * (1 - alpha / 2))\n",
    "    stderr = np.std(boot_stats)\n",
    "    \n",
    "    return {\n",
    "        'estimate': estimate,\n",
    "        'lower': lower,\n",
    "        'upper': upper,\n",
    "        'stderr': stderr\n",
    "    }\n",
    "\n",
    "def fit_linear_trend(x, y):\n",
    "    \"\"\"\n",
    "    Fit linear trend model and return comprehensive results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        Independent variable (e.g., years)\n",
    "    y : array-like\n",
    "        Dependent variable (e.g., indicator values)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Model results including slope, intercept, RÂ², p-value, fitted values, residuals\n",
    "    \"\"\"\n",
    "    # Fit linear model\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "    \n",
    "    # Predict values\n",
    "    y_pred = slope * x + intercept\n",
    "    residuals = y - y_pred\n",
    "    \n",
    "    # Compute 95% CI for slope\n",
    "    n = len(x)\n",
    "    t_val = stats.t.ppf(0.975, n - 2)  # 95% two-tailed\n",
    "    slope_ci = (slope - t_val * std_err, slope + t_val * std_err)\n",
    "    \n",
    "    return {\n",
    "        'slope': slope,\n",
    "        'intercept': intercept,\n",
    "        'r_squared': r_value**2,\n",
    "        'p_value': p_value,\n",
    "        'std_err': std_err,\n",
    "        'slope_ci': slope_ci,\n",
    "        'residuals': residuals,\n",
    "        'fitted': y_pred\n",
    "    }\n",
    "\n",
    "def check_assumptions(residuals, fitted):\n",
    "    \"\"\"\n",
    "    Check linear regression assumptions.\n",
    "    \n",
    "    Returns diagnostic statistics for:\n",
    "    - Normality (Shapiro-Wilk test)\n",
    "    - Homoscedasticity (correlation between |residuals| and fitted)\n",
    "    - Independence (Durbin-Watson statistic)\n",
    "    \"\"\"\n",
    "    # Normality\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(residuals)\n",
    "    \n",
    "    # Homoscedasticity\n",
    "    abs_resid = np.abs(residuals)\n",
    "    het_corr, het_p = stats.pearsonr(fitted, abs_resid)\n",
    "    \n",
    "    # Autocorrelation (Durbin-Watson)\n",
    "    dw = np.sum(np.diff(residuals)**2) / np.sum(residuals**2)\n",
    "    \n",
    "    return {\n",
    "        'normality': {'statistic': shapiro_stat, 'p_value': shapiro_p, 'pass': shapiro_p > 0.05},\n",
    "        'homoscedasticity': {'correlation': het_corr, 'p_value': het_p, 'pass': het_p > 0.05},\n",
    "        'independence': {'durbin_watson': dw, 'pass': 1.5 < dw < 2.5}\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Statistical utilities defined\")\n",
    "\n",
    "# ============================================================================\n",
    "# PLOTTING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def plot_temporal_trend(x_data, y_data, ci_lower, ci_upper, y_trend,\n",
    "                       xlabel, ylabel, title, filename, color='steelblue',\n",
    "                       trend_info=None):\n",
    "    \"\"\"\n",
    "    Create publication-quality temporal trend plot.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_data : array-like\n",
    "        X-axis values (years)\n",
    "    y_data : array-like\n",
    "        Y-axis values (indicator estimates)\n",
    "    ci_lower, ci_upper : array-like\n",
    "        Confidence interval bounds\n",
    "    y_trend : array-like\n",
    "        Fitted trend line values\n",
    "    xlabel, ylabel, title : str\n",
    "        Axis labels and title\n",
    "    filename : str\n",
    "        Path to save figure\n",
    "    color : str\n",
    "        Color for data points and CI\n",
    "    trend_info : dict, optional\n",
    "        Dictionary with slope, r_squared, p_value for text box\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot data and CI\n",
    "    ax.plot(x_data, y_data, 'o-', linewidth=2.5, markersize=10, \n",
    "            label='Observed', color=color, markeredgecolor='black', markeredgewidth=1)\n",
    "    ax.fill_between(x_data, ci_lower, ci_upper, alpha=0.25, color=color, \n",
    "                     label='95% CI (Bootstrap)')\n",
    "    \n",
    "    # Plot trend line\n",
    "    ax.plot(x_data, y_trend, '--', linewidth=2.5, color='red', label='Linear Trend')\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel(xlabel, fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel(ylabel, fontsize=13, fontweight='bold')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add trend info box if provided\n",
    "    if trend_info:\n",
    "        textstr = f'Slope: {trend_info[\"slope\"]:+.4g}\\n'\n",
    "        textstr += f'RÂ² = {trend_info[\"r_squared\"]:.3f}\\n'\n",
    "        textstr += f'p = {trend_info[\"p_value\"]:.4f}'\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=11,\n",
    "                verticalalignment='top', bbox=props)\n",
    "    \n",
    "    ax.legend(frameon=True, loc='upper right', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(x_data.min() - 0.5, x_data.max() + 0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ“ Figure saved to {filename}\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"âœ“ Plotting utilities defined\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL FUNCTIONS LOADED SUCCESSFULLY\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf53493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load data and filter to complete years (2015-2024)\n",
    "Excludes partial years 2014 and 2025\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load raw data\n",
    "data = load_excel_data('../data/raw/Observations 2012-2025.xlsx')\n",
    "df_raw = data['observations']\n",
    "\n",
    "print(f\"\\nRaw data loaded: {len(df_raw)} observations\")\n",
    "\n",
    "# Clean data\n",
    "df = clean_observations(df_raw)\n",
    "\n",
    "print(f\"After cleaning: {len(df)} observations\")\n",
    "\n",
    "# Filter to complete years only (2015-2024)\n",
    "df_complete = df[df['year'].between(2015, 2024)].copy()\n",
    "\n",
    "print(f\"After filtering to 2015-2024: {len(df_complete)} observations\")\n",
    "print(f\"Years included: {sorted(df_complete['year'].unique())}\")\n",
    "\n",
    "# Verify data quality\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Unique species: {df_complete['species_name'].nunique()}\")\n",
    "print(f\"Unique transects: {df_complete['transect_name'].nunique()}\")\n",
    "print(f\"Unique observers: {df_complete['observer_name'].nunique()}\")\n",
    "print(f\"Total birds counted: {df_complete['individual_count'].sum():,.0f}\")\n",
    "print(f\"Mean count per observation: {df_complete['individual_count'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e1079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute bootstrap confidence intervals for Total Abundance\n",
    "Uses 1000 bootstrap iterations per year\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BOOTSTRAP CIs: TOTAL ABUNDANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def total_abundance_stat(data):\n",
    "    \"\"\"Statistic function for bootstrap\"\"\"\n",
    "    return data['individual_count'].sum()\n",
    "\n",
    "n_boot = 1000\n",
    "confidence_level = 0.95\n",
    "\n",
    "total_abundance_cis = []\n",
    "\n",
    "for year in years:\n",
    "    print(f\"\\nBootstrapping {year}...\", end=\" \")\n",
    "    year_data = df_complete[df_complete['year'] == year]\n",
    "    \n",
    "    # Bootstrap\n",
    "    boot_estimates = []\n",
    "    n = len(year_data)\n",
    "    \n",
    "    for _ in range(n_boot):\n",
    "        # Resample observations with replacement\n",
    "        sample = year_data.sample(n=n, replace=True)\n",
    "        boot_estimates.append(total_abundance_stat(sample))\n",
    "    \n",
    "    boot_estimates = np.array(boot_estimates)\n",
    "    \n",
    "    # Compute CI\n",
    "    alpha = 1 - confidence_level\n",
    "    estimate = total_abundance_stat(year_data)\n",
    "    ci_lower = np.percentile(boot_estimates, 100 * alpha / 2)\n",
    "    ci_upper = np.percentile(boot_estimates, 100 * (1 - alpha / 2))\n",
    "    stderr = np.std(boot_estimates)\n",
    "    \n",
    "    total_abundance_cis.append({\n",
    "        'year': year,\n",
    "        'estimate': estimate,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'stderr': stderr,\n",
    "        'ci_width': ci_upper - ci_lower\n",
    "    })\n",
    "    \n",
    "    print(f\"âœ“ [{ci_lower:,.0f}, {ci_upper:,.0f}]\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "total_abundance_ci_df = pd.DataFrame(total_abundance_cis)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CI Summary:\")\n",
    "print(total_abundance_ci_df[['year', 'estimate', 'ci_lower', 'ci_upper', 'ci_width']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67005d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fit linear trend model for Total Abundance\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEMPORAL TREND: TOTAL ABUNDANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare data\n",
    "x = np.array(years)\n",
    "y = total_abundance_ci_df['estimate'].values\n",
    "\n",
    "# Fit linear model\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "\n",
    "# Compute 95% CI for slope\n",
    "n = len(x)\n",
    "t_val = stats.t.ppf(0.975, n - 2)  # 95% CI\n",
    "slope_ci_lower = slope - t_val * std_err\n",
    "slope_ci_upper = slope + t_val * std_err\n",
    "\n",
    "# Predictions\n",
    "y_pred = slope * x + intercept\n",
    "residuals = y - y_pred\n",
    "\n",
    "# Store results\n",
    "total_abundance_trend = {\n",
    "    'slope': slope,\n",
    "    'intercept': intercept,\n",
    "    'r_squared': r_value**2,\n",
    "    'p_value': p_value,\n",
    "    'slope_ci': (slope_ci_lower, slope_ci_upper),\n",
    "    'residuals': residuals,\n",
    "    'fitted': y_pred\n",
    "}\n",
    "\n",
    "print(f\"\\nLinear Trend Results:\")\n",
    "print(f\"  Slope: {slope:+.2f} individuals/year\")\n",
    "print(f\"  95% CI: [{slope_ci_lower:+.2f}, {slope_ci_upper:+.2f}]\")\n",
    "print(f\"  RÂ² = {r_value**2:.4f}\")\n",
    "print(f\"  p-value = {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.001:\n",
    "    sig_str = \"p < 0.001 ***\"\n",
    "elif p_value < 0.01:\n",
    "    sig_str = \"p < 0.01 **\"\n",
    "elif p_value < 0.05:\n",
    "    sig_str = \"p < 0.05 *\"\n",
    "else:\n",
    "    sig_str = \"p â‰¥ 0.05 (not significant)\"\n",
    "\n",
    "print(f\"  Significance: {sig_str}\")\n",
    "\n",
    "# Interpret trend direction\n",
    "if slope > 0:\n",
    "    direction = \"INCREASING\"\n",
    "elif slope < 0:\n",
    "    direction = \"DECREASING\"\n",
    "else:\n",
    "    direction = \"STABLE\"\n",
    "\n",
    "print(f\"\\n  â†’ Trend direction: {direction}\")\n",
    "print(f\"  â†’ Total change 2015-2024: {slope * 9:+,.0f} individuals ({slope * 9 / y[0] * 100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74563e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check linear regression assumptions for Total Abundance model\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ASSUMPTION CHECKS: TOTAL ABUNDANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Normality of residuals (Shapiro-Wilk test)\n",
    "shapiro_stat, shapiro_p = stats.shapiro(residuals)\n",
    "print(f\"\\n1. Normality (Shapiro-Wilk test):\")\n",
    "print(f\"   Statistic = {shapiro_stat:.4f}, p-value = {shapiro_p:.4f}\")\n",
    "if shapiro_p > 0.05:\n",
    "    print(f\"   âœ“ Residuals appear normally distributed\")\n",
    "else:\n",
    "    print(f\"   âš  Residuals may not be normally distributed\")\n",
    "\n",
    "# 2. Homoscedasticity (check correlation between |residuals| and fitted)\n",
    "abs_resid = np.abs(residuals)\n",
    "het_corr, het_p = stats.pearsonr(y_pred, abs_resid)\n",
    "print(f\"\\n2. Homoscedasticity:\")\n",
    "print(f\"   Correlation(fitted, |residuals|) = {het_corr:.4f}, p-value = {het_p:.4f}\")\n",
    "if het_p > 0.05:\n",
    "    print(f\"   âœ“ Variance appears constant\")\n",
    "else:\n",
    "    print(f\"   âš  Possible heteroscedasticity\")\n",
    "\n",
    "# 3. Independence (Durbin-Watson statistic)\n",
    "dw = np.sum(np.diff(residuals)**2) / np.sum(residuals**2)\n",
    "print(f\"\\n3. Independence (Durbin-Watson):\")\n",
    "print(f\"   DW statistic = {dw:.4f}\")\n",
    "if 1.5 < dw < 2.5:\n",
    "    print(f\"   âœ“ No strong autocorrelation detected\")\n",
    "else:\n",
    "    print(f\"   âš  Possible autocorrelation (DW should be near 2)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6633d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute bootstrap confidence intervals for Mean Abundance per Point\n",
    "Uses 1000 bootstrap iterations per year\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BOOTSTRAP CIs: MEAN ABUNDANCE PER POINT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def mean_abundance_stat(data):\n",
    "    \"\"\"Statistic function for bootstrap (mean abundance per point).\"\"\"\n",
    "    return compute_mean_abundance_per_point(data)\n",
    "\n",
    "n_boot = 1000\n",
    "confidence_level = 0.95\n",
    "\n",
    "mean_abundance_cis = []\n",
    "\n",
    "for year in years:\n",
    "    print(f\"\\nBootstrapping {year}...\", end=\" \")\n",
    "    year_data = df_complete[df_complete['year'] == year]\n",
    "\n",
    "    # Bootstrap\n",
    "    boot_estimates = []\n",
    "    n = len(year_data)\n",
    "\n",
    "    for _ in range(n_boot):\n",
    "        # Resample observations with replacement\n",
    "        sample = year_data.sample(n=n, replace=True)\n",
    "        boot_estimates.append(mean_abundance_stat(sample))\n",
    "\n",
    "    boot_estimates = np.array(boot_estimates)\n",
    "\n",
    "    # Compute CI\n",
    "    alpha = 1 - confidence_level\n",
    "    estimate = mean_abundance_stat(year_data)\n",
    "    ci_lower = np.percentile(boot_estimates, 100 * alpha / 2)\n",
    "    ci_upper = np.percentile(boot_estimates, 100 * (1 - alpha / 2))\n",
    "    stderr = np.std(boot_estimates)\n",
    "\n",
    "    mean_abundance_cis.append({\n",
    "        'year': year,\n",
    "        'estimate': estimate,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'stderr': stderr,\n",
    "        'ci_width': ci_upper - ci_lower\n",
    "    })\n",
    "\n",
    "    print(f\"âœ“ [{ci_lower:.2f}, {ci_upper:.2f}]\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "mean_abundance_ci_df = pd.DataFrame(mean_abundance_cis)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CI Summary:\")\n",
    "print(mean_abundance_ci_df[['year', 'estimate', 'ci_lower', 'ci_upper', 'ci_width']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412dc7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fit linear trend model for Mean Abundance per Point\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEMPORAL TREND: MEAN ABUNDANCE PER POINT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare data\n",
    "x = np.array(years)\n",
    "y = mean_abundance_ci_df['estimate'].values\n",
    "\n",
    "# Fit linear model\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "\n",
    "# Compute 95% CI for slope\n",
    "slope_ci_lower = slope - t_val * std_err\n",
    "slope_ci_upper = slope + t_val * std_err\n",
    "\n",
    "# Predictions\n",
    "y_pred = slope * x + intercept\n",
    "residuals = y - y_pred\n",
    "\n",
    "# Store results\n",
    "mean_abundance_trend = {\n",
    "    'slope': slope,\n",
    "    'intercept': intercept,\n",
    "    'r_squared': r_value**2,\n",
    "    'p_value': p_value,\n",
    "    'slope_ci': (slope_ci_lower, slope_ci_upper),\n",
    "    'residuals': residuals,\n",
    "    'fitted': y_pred\n",
    "}\n",
    "\n",
    "print(f\"\\nLinear Trend Results:\")\n",
    "print(f\"  Slope: {slope:+.4f} individuals/point/year\")\n",
    "print(f\"  95% CI: [{slope_ci_lower:+.4f}, {slope_ci_upper:+.4f}]\")\n",
    "print(f\"  RÂ² = {r_value**2:.4f}\")\n",
    "print(f\"  p-value = {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.001:\n",
    "    sig_str = \"p < 0.001 ***\"\n",
    "elif p_value < 0.01:\n",
    "    sig_str = \"p < 0.01 **\"\n",
    "elif p_value < 0.05:\n",
    "    sig_str = \"p < 0.05 *\"\n",
    "else:\n",
    "    sig_str = \"p â‰¥ 0.05 (not significant)\"\n",
    "\n",
    "print(f\"  Significance: {sig_str}\")\n",
    "\n",
    "# Interpret trend direction\n",
    "if slope > 0:\n",
    "    direction = \"INCREASING\"\n",
    "elif slope < 0:\n",
    "    direction = \"DECREASING\"\n",
    "else:\n",
    "    direction = \"STABLE\"\n",
    "\n",
    "print(f\"\\n  â†’ Trend direction: {direction}\")\n",
    "print(f\"  â†’ Total change 2015-2024: {slope * 9:+.4f} individuals/point ({slope * 9 / y[0] * 100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64df8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check linear regression assumptions for Mean Abundance per Point model\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ASSUMPTION CHECKS: MEAN ABUNDANCE PER POINT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Normality\n",
    "shapiro_stat, shapiro_p = stats.shapiro(residuals)\n",
    "print(f\"\\n1. Normality (Shapiro-Wilk test):\")\n",
    "print(f\"   Statistic = {shapiro_stat:.4f}, p-value = {shapiro_p:.4f}\")\n",
    "if shapiro_p > 0.05:\n",
    "    print(f\"   âœ“ Residuals appear normally distributed\")\n",
    "else:\n",
    "    print(f\"   âš  Residuals may not be normally distributed\")\n",
    "\n",
    "# 2. Homoscedasticity\n",
    "abs_resid = np.abs(residuals)\n",
    "het_corr, het_p = stats.pearsonr(y_pred, abs_resid)\n",
    "print(f\"\\n2. Homoscedasticity:\")\n",
    "print(f\"   Correlation(fitted, |residuals|) = {het_corr:.4f}, p-value = {het_p:.4f}\")\n",
    "if het_p > 0.05:\n",
    "    print(f\"   âœ“ Variance appears constant\")\n",
    "else:\n",
    "    print(f\"   âš  Possible heteroscedasticity\")\n",
    "\n",
    "# 3. Independence\n",
    "dw = np.sum(np.diff(residuals)**2) / np.sum(residuals**2)\n",
    "print(f\"\\n3. Independence (Durbin-Watson):\")\n",
    "print(f\"   DW statistic = {dw:.4f}\")\n",
    "if 1.5 < dw < 2.5:\n",
    "    print(f\"   âœ“ No strong autocorrelation detected\")\n",
    "else:\n",
    "    print(f\"   âš  Possible autocorrelation\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9736677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create diagnostic plots for both models\n",
    "\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# === TOTAL ABUNDANCE DIAGNOSTICS ===\n",
    "residuals_total = total_abundance_trend['residuals']\n",
    "fitted_total = total_abundance_trend['fitted']\n",
    "\n",
    "# Residuals vs Fitted\n",
    "axes[0, 0].scatter(fitted_total, residuals_total, s=100, alpha=0.7, edgecolors='black')\n",
    "axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Fitted Values', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Residuals', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_title('Total Abundance: Residuals vs Fitted', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q Plot\n",
    "stats.probplot(residuals_total, dist=\"norm\", plot=axes[0, 1])\n",
    "axes[0, 1].set_title('Total Abundance: Q-Q Plot', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# === MEAN ABUNDANCE PER POINT DIAGNOSTICS ===\n",
    "residuals_mean = mean_abundance_trend['residuals']\n",
    "fitted_mean = mean_abundance_trend['fitted']\n",
    "\n",
    "# Residuals vs Fitted\n",
    "axes[1, 0].scatter(fitted_mean, residuals_mean, s=100, alpha=0.7, edgecolors='black', color='green')\n",
    "axes[1, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Fitted Values', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Residuals', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_title('Mean Abundance per Point: Residuals vs Fitted', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q Plot\n",
    "stats.probplot(residuals_mean, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Mean Abundance per Point: Q-Q Plot', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].get_lines()[0].set_color('green')\n",
    "axes[1, 1].get_lines()[1].set_color('red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/diagnostics_person_B.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ“ Diagnostic plots saved to ../figures/diagnostics_person_B.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df00983",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create publication-quality figure for Total Abundance\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Extract data\n",
    "x_data = total_abundance_ci_df['year'].values\n",
    "y_data = total_abundance_ci_df['estimate'].values\n",
    "ci_lower = total_abundance_ci_df['ci_lower'].values\n",
    "ci_upper = total_abundance_ci_df['ci_upper'].values\n",
    "\n",
    "# Plot data points and confidence interval\n",
    "ax.plot(x_data, y_data, 'o-', linewidth=2.5, markersize=10, \n",
    "        label='Observed', color='steelblue', markeredgecolor='black', markeredgewidth=1)\n",
    "ax.fill_between(x_data, ci_lower, ci_upper, alpha=0.25, color='steelblue', \n",
    "                 label='95% CI (Bootstrap)')\n",
    "\n",
    "# Plot trend line\n",
    "x_trend = np.array(years)\n",
    "y_trend = total_abundance_trend['fitted']\n",
    "ax.plot(x_trend, y_trend, '--', linewidth=2.5, color='red', label='Linear Trend')\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Year', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Total Abundance (individuals)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Temporal Trend in Total Abundance (2015-2024)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add trend statistics box\n",
    "textstr = f'Slope: {total_abundance_trend[\"slope\"]:+.1f} ind/year\\n'\n",
    "textstr += f'RÂ² = {total_abundance_trend[\"r_squared\"]:.3f}\\n'\n",
    "textstr += f'p = {total_abundance_trend[\"p_value\"]:.4f}'\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=11,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "ax.legend(frameon=True, loc='upper right', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(2014.5, 2024.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/indicator_total_abundance.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ“ Figure saved to ../figures/indicator_total_abundance.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create publication-quality figure for Mean Abundance per Point\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Extract data\n",
    "x_data = mean_abundance_ci_df['year'].values\n",
    "y_data = mean_abundance_ci_df['estimate'].values\n",
    "ci_lower = mean_abundance_ci_df['ci_lower'].values\n",
    "ci_upper = mean_abundance_ci_df['ci_upper'].values\n",
    "\n",
    "# Plot data points and confidence interval\n",
    "ax.plot(x_data, y_data, 'o-', linewidth=2.5, markersize=10, \n",
    "        label='Observed', color='darkgreen', markeredgecolor='black', markeredgewidth=1)\n",
    "ax.fill_between(x_data, ci_lower, ci_upper, alpha=0.25, color='darkgreen', \n",
    "                 label='95% CI (Bootstrap)')\n",
    "\n",
    "# Plot trend line\n",
    "x_trend = np.array(years)\n",
    "y_trend = mean_abundance_trend['fitted']\n",
    "ax.plot(x_trend, y_trend, '--', linewidth=2.5, color='red', label='Linear Trend')\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Year', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Mean Abundance per Point (individuals/point)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Temporal Trend in Mean Abundance per Point (2015-2024)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add trend statistics box\n",
    "textstr = f'Slope: {mean_abundance_trend[\"slope\"]:+.4f} ind/pt/yr\\n'\n",
    "textstr += f'RÂ² = {mean_abundance_trend[\"r_squared\"]:.3f}\\n'\n",
    "textstr += f'p = {mean_abundance_trend[\"p_value\"]:.4f}'\n",
    "props = dict(boxstyle='round', facecolor='lightgreen', alpha=0.8)\n",
    "ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=11,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "ax.legend(frameon=True, loc='upper right', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(2014.5, 2024.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/indicator_mean_abundance_per_point.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ“ Figure saved to ../figures/indicator_mean_abundance_per_point.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29d36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INTERPRETATION PARAGRAPHS FOR REPORT\n",
    "Copy these to your report and fill in the bracketed placeholders with actual values\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INTERPRETATION: TOTAL ABUNDANCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Resolve numpy.float64 index issue by casting to int\n",
    "direction_total = ['declining', 'stable', 'increasing'][int(np.sign(total_abundance_trend['slope'])) + 1]\n",
    "\n",
    "direction_mean = ['declining', 'stable', 'increasing'][int(np.sign(mean_abundance_trend['slope'])) + 1]\n",
    "\n",
    "interpretation_total = f\"\"\"\n",
    "**Total Abundance Temporal Trend**\n",
    "\n",
    "Total abundance across the bird community exhibited a {direction_total} \n",
    "trend over the study period 2015-2024 (slope = {total_abundance_trend['slope']:.2f} individuals/year, \n",
    "95% CI: [{total_abundance_trend['slope_ci'][0]:.2f}, {total_abundance_trend['slope_ci'][1]:.2f}], \n",
    "RÂ² = {total_abundance_trend['r_squared']:.3f}, p = {total_abundance_trend['p_value']:.4f}). \n",
    "The linear model explained {total_abundance_trend['r_squared']*100:.1f}% of the variance in annual abundance estimates. \n",
    "Over the 9-year period, total abundance changed by approximately {total_abundance_trend['slope'] * 9:+,.0f} individuals \n",
    "({total_abundance_trend['slope'] * 9 / total_abundance_ci_df.iloc[0]['estimate'] * 100:+.1f}% relative to 2015 baseline).\n",
    "\n",
    "Bootstrap confidence intervals (1000 iterations) quantified estimation uncertainty arising from sampling variability. \n",
    "The width of confidence intervals varied across years, likely reflecting differences in sampling effort and species \n",
    "composition heterogeneity. Diagnostic checks revealed [FILL IN: e.g., \"residuals were approximately normally distributed \n",
    "(Shapiro-Wilk p = {shapiro_p:.3f}) with no evidence of heteroscedasticity or autocorrelation\"].\n",
    "\n",
    "The {'positive' if total_abundance_trend['slope'] > 0 else 'negative'} trend in total abundance suggests \n",
    "[FILL IN BIOLOGICAL INTERPRETATION: e.g., \"overall population growth across the community\" or \n",
    "\"potential declines driven by decreasing abundances of common species\"]. This pattern must be interpreted \n",
    "cautiously given [DISCUSS: observer effects (one observer conducted 36% of surveys), \n",
    "potential changes in detection probability over time, or relationship to sampling effort trends from Person C]. \n",
    "Comparison with species richness trends from Person A will reveal whether abundance changes reflect \n",
    "community-wide shifts versus dominance by particular species.\n",
    "\"\"\"\n",
    "\n",
    "print(interpretation_total)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INTERPRETATION: MEAN ABUNDANCE PER POINT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "interpretation_mean = f\"\"\"\n",
    "**Mean Abundance per Point (Effort-Standardized)**\n",
    "\n",
    "To account for variable sampling effort across years, we computed mean abundance per observation point, \n",
    "standardizing total counts by the number of unique transect-point combinations surveyed each year. \n",
    "This effort-corrected metric showed a {direction_mean} \n",
    "trend (slope = {mean_abundance_trend['slope']:.4f} individuals/point/year, \n",
    "95% CI: [{mean_abundance_trend['slope_ci'][0]:.4f}, {mean_abundance_trend['slope_ci'][1]:.4f}], \n",
    "RÂ² = {mean_abundance_trend['r_squared']:.3f}, p = {mean_abundance_trend['p_value']:.4f}).\n",
    "\n",
    "Comparing total abundance and mean per-point trends reveals [FILL IN COMPARISON: \n",
    "e.g., \"both metrics show similar directional trends, suggesting genuine ecological change rather than sampling artifacts\" \n",
    "OR \"divergent patterns, where total abundance increases but mean per point declines, indicating that increased sampling \n",
    "effort rather than higher densities drives the total abundance pattern\"]. \n",
    "\n",
    "The effort-standardized metric provides a more robust assessment of actual bird densities at surveyed locations, \n",
    "controlling for the confounding effect of varying numbers of observation points across years. \n",
    "[FILL IN AFTER SEEING PERSON C's EFFORT DATA: e.g., \"Given that Person C found observer effort varied \n",
    "substantially (range: X-Y points per year), this standardization is critical for valid trend interpretation\"].\n",
    "\n",
    "Model diagnostics indicate [DESCRIBE ASSUMPTION CHECKS]. One limitation of this metric is that it assumes \n",
    "equal detectability across all points and years. If detection probability varied systematically \n",
    "(e.g., due to observer skill improvements or habitat changes affecting sightability), the trends may partly \n",
    "reflect these methodological factors rather than true population dynamics.\n",
    "\n",
    "[FILL IN AFTER COMPARING TO PERSON A: \"Integrating with Person A's diversity metrics, we observe that \n",
    "[e.g., abundance â†‘ while richness â†’ suggesting dominance by fewer species, OR abundance â†’ while richness â†‘ \n",
    "suggesting community restructuring with evenness shifts]\"].\n",
    "\"\"\"\n",
    "\n",
    "print(interpretation_mean)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SYNTHESIS NOTES FOR SECTION 4\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "synthesis = \"\"\"\n",
    "**Key Points for Integration:**\n",
    "\n",
    "1. **Methodological Insights:**\n",
    "   - Effort standardization matters: raw vs. per-point metrics may tell different stories\n",
    "   - Bootstrap CIs reveal realistic uncertainty in annual estimates\n",
    "   - Linear trends provide interpretable effect sizes (individuals/year or individuals/point/year)\n",
    "\n",
    "2. **Ecological Interpretations:**\n",
    "   - Abundance trends reflect NET change across all species (increases + decreases)\n",
    "   - Must compare to richness (Person A) to understand community composition shifts\n",
    "   - Must compare to effort (Person C) to validate detection probability assumptions\n",
    "\n",
    "3. **Limitations:**\n",
    "   - Cannot distinguish species-specific contributions to abundance trends (wait for Section 3)\n",
    "   - Detection probability assumed constant (may vary with observer, weather, time of day)\n",
    "   - Observer effect (CONDE Beatriz 36% of surveys) may introduce bias\n",
    "   - Spatial coverage variation may affect representativeness\n",
    "\n",
    "4. **Recommendations:**\n",
    "   - Continue monitoring with consistent effort levels to improve trend detection\n",
    "   - Record detection distances to enable distance sampling corrections\n",
    "   - Balance observer workload to reduce individual-specific biases\n",
    "   - Consider species-specific analyses to identify drivers of community-level trends\n",
    "\"\"\"\n",
    "\n",
    "print(synthesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f829c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up plotting style for publication quality\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cecf411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Loading Functions\n",
    "def load_excel_data(filepath):\n",
    "    filepath = Path(filepath)\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"Can't find data file: {filepath}\")\n",
    "    \n",
    "    print(f\"Loading data from {filepath}\")\n",
    "    \n",
    "    # Load ESPECES sheet - skip bad headers, manually name columns\n",
    "    df_species = pd.read_excel(filepath, sheet_name='ESPECES', header=None, skiprows=1)\n",
    "    df_species.columns = ['_empty1', '_empty2', 'french_name', 'scientific_name', 'status']\n",
    "    df_species = df_species[['french_name', 'scientific_name', 'status']].copy()\n",
    "    \n",
    "    # clean whitespace\n",
    "    for col in df_species.columns:\n",
    "        if df_species[col].dtype == 'object':\n",
    "            df_species[col] = df_species[col].str.strip()\n",
    "    \n",
    "    print(f\"Loaded {len(df_species)} species\")\n",
    "    \n",
    "    # Load GPS-MILIEU sheet\n",
    "    df_gps = pd.read_excel(filepath, sheet_name='GPS-MILIEU', header=None, skiprows=1)\n",
    "    df_gps.columns = ['_empty1', '_empty2', 'transect_name', 'gps_x', 'gps_y', \n",
    "                      'habitat_type', 'site_id', 'point_id']\n",
    "    df_gps = df_gps[['transect_name', 'habitat_type', 'site_id', 'point_id']].copy()\n",
    "    \n",
    "    # clean whitespace\n",
    "    for col in df_gps.columns:\n",
    "        if df_gps[col].dtype == 'object':\n",
    "            df_gps[col] = df_gps[col].str.strip()\n",
    "    \n",
    "    print(f\"Loaded {len(df_gps)} GPS points\")\n",
    "    \n",
    "    # Load main observations sheet\n",
    "    df_obs = pd.read_excel(filepath, sheet_name='NOM FRANÃ‡AIS')\n",
    "    \n",
    "    # rename to something sensible\n",
    "    column_mapping = {\n",
    "        'Nom observateur': 'observer_name',\n",
    "        'code dÃ©partement': 'department_code',\n",
    "        'Nom transect': 'transect_name',\n",
    "        'date': 'date',\n",
    "        '1er, 2e ou 3e passage': 'visit_number',\n",
    "        'nuages': 'cloud_cover_raw',\n",
    "        'pluie': 'rain',\n",
    "        'vent': 'wind',\n",
    "        'visibilitÃ©': 'visibility',\n",
    "        'NÂ° point': 'point_number',\n",
    "        'heure dÃ©but': 'start_time',\n",
    "        'ESPECE': 'species_name',\n",
    "        'distances de contact': 'distance_category_raw',\n",
    "        'totaux': 'count_auditory',\n",
    "        'Unnamed: 22': 'count_visual_no_flight',\n",
    "        'Unnamed: 23': 'count_audio_visual_no_flight',\n",
    "        'Unnamed: 24': 'count_audio_visual_flight',\n",
    "        'Unnamed: 25': 'notes'\n",
    "    }\n",
    "    \n",
    "    df_obs = df_obs.rename(columns=column_mapping)\n",
    "    \n",
    "    # convert the count columns to numeric (they're mixed type with headers in row 1)\n",
    "    print(\"Converting count columns to numeric\")\n",
    "    count_cols = ['count_auditory', 'count_visual_no_flight', \n",
    "                  'count_audio_visual_no_flight', 'count_audio_visual_flight']\n",
    "    \n",
    "    for col in count_cols:\n",
    "        df_obs[col] = pd.to_numeric(df_obs[col], errors='coerce')\n",
    "    \n",
    "    # sum across all detection methods to get total count\n",
    "    df_obs['individual_count'] = df_obs[count_cols].sum(axis=1)\n",
    "    \n",
    "    # convert date properly\n",
    "    df_obs['date'] = pd.to_datetime(df_obs['date'], errors='coerce')\n",
    "    df_obs['year'] = df_obs['date'].dt.year\n",
    "    \n",
    "    # clean whitespace from text columns\n",
    "    for col in ['observer_name', 'transect_name', 'species_name', 'start_time', 'notes']:\n",
    "        if col in df_obs.columns:\n",
    "            df_obs[col] = df_obs[col].astype(str).str.strip()\n",
    "    \n",
    "    print(f\"Loaded {len(df_obs)} observation records\")\n",
    "    \n",
    "    return {\n",
    "        'observations': df_obs,\n",
    "        'species': df_species,\n",
    "        'gps': df_gps\n",
    "    }\n",
    "\n",
    "def clean_observations(df):\n",
    "    # Clean and validate observations.\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    print(\"Data Cleaning\")\n",
    "    print(f\"Starting with {len(df_clean)} records\")\n",
    "    \n",
    "    # fix negative wind values\n",
    "    if 'wind' in df_clean.columns:\n",
    "        n_negative = (df_clean['wind'] < 0).sum()\n",
    "        if n_negative > 0:\n",
    "            print(f\"âš  Found {n_negative} negative wind values - setting to NaN\")\n",
    "            df_clean.loc[df_clean['wind'] < 0, 'wind'] = np.nan\n",
    "    \n",
    "    # remove zero/negative counts\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean['individual_count'] > 0]\n",
    "    removed = before - len(df_clean)\n",
    "    if removed > 0:\n",
    "        print(f\"Removed {removed} records with zero/negative counts\")\n",
    "    \n",
    "    # remove records missing essential fields\n",
    "    essential_cols = ['year', 'species_name', 'individual_count', 'transect_name']\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean.dropna(subset=essential_cols)\n",
    "    removed = before - len(df_clean)\n",
    "    if removed > 0:\n",
    "        print(f\"Removed {removed} records with missing essential data\")\n",
    "    \n",
    "    # create observation ID\n",
    "    df_clean['observation_id'] = range(1, len(df_clean) + 1)\n",
    "    \n",
    "    # summary\n",
    "    print(f\"Final dataset: {len(df_clean)} records\")\n",
    "    print(f\"  Years: {df_clean['year'].min()} - {df_clean['year'].max()}\")\n",
    "    print(f\"  Unique species: {df_clean['species_name'].nunique()}\")\n",
    "    print(f\"  Unique transects: {df_clean['transect_name'].nunique()}\")\n",
    "    print(f\"  Unique observers: {df_clean['observer_name'].nunique()}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "print(\"âœ… Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97637696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load and Clean Data\n",
    "print(\"ðŸ“Š Loading and cleaning data...\")\n",
    "data_dict = load_excel_data('/Applications/Documents/app-stats/birds-biodiversity/data/raw/Observations 2012-2025.xlsx')\n",
    "df_clean = clean_observations(data_dict['observations'])\n",
    "\n",
    "print(f\"\\nâœ… Data loaded successfully:\")\n",
    "print(f\"   - {len(df_clean)} clean observation records\")\n",
    "print(f\"   - Years: {df_clean['year'].min()} to {df_clean['year'].max()}\")\n",
    "print(f\"   - {df_clean['transect_name'].nunique()} unique transects\")\n",
    "print(f\"   - {df_clean['species_name'].nunique()} unique species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d0364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Bootstrap Confidence Intervals\n",
    "def bootstrap_sampling_metrics(df_clean, n_bootstrap=1000, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate bootstrap confidence intervals for sampling metrics\n",
    "    \"\"\"\n",
    "    print(\"Calculating bootstrap confidence intervals...\")\n",
    "    \n",
    "    years = sorted(df_clean[df_clean['year'].between(2015, 2024)]['year'].unique())\n",
    "    bootstrap_results = []\n",
    "    \n",
    "    for year in years:\n",
    "        year_data = df_clean[df_clean['year'] == year]\n",
    "        \n",
    "        boot_effort = []\n",
    "        boot_transects = []\n",
    "        boot_coverage = []\n",
    "        \n",
    "        for _ in range(n_bootstrap):\n",
    "            # Resample observations with replacement\n",
    "            bootstrap_sample = year_data.sample(n=len(year_data), replace=True)\n",
    "            \n",
    "            # Calculate metrics for bootstrap sample\n",
    "            boot_effort.append(len(bootstrap_sample))\n",
    "            boot_transects.append(bootstrap_sample['transect_name'].nunique())\n",
    "            \n",
    "            # For coverage percentage\n",
    "            total_transects = df_clean['transect_name'].nunique()\n",
    "            coverage_pct = (bootstrap_sample['transect_name'].nunique() / total_transects * 100)\n",
    "            boot_coverage.append(coverage_pct)\n",
    "        \n",
    "        # Calculate confidence intervals\n",
    "        alpha = (1 - confidence) / 2\n",
    "        ci_lower = alpha * 100\n",
    "        ci_upper = (1 - alpha) * 100\n",
    "        \n",
    "        bootstrap_results.append({\n",
    "            'year': year,\n",
    "            'effort_lower': np.percentile(boot_effort, ci_lower),\n",
    "            'effort_upper': np.percentile(boot_effort, ci_upper),\n",
    "            'transects_lower': np.percentile(boot_transects, ci_lower),\n",
    "            'transects_upper': np.percentile(boot_transects, ci_upper),\n",
    "            'coverage_lower': np.percentile(boot_coverage, ci_lower),\n",
    "            'coverage_upper': np.percentile(boot_coverage, ci_upper)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(bootstrap_results)\n",
    "\n",
    "# Execute bootstrap\n",
    "bootstrap_df = bootstrap_sampling_metrics(df_clean)\n",
    "print(\"âœ… Bootstrap confidence intervals calculated\")\n",
    "print(bootstrap_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb7e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Model Assumption Checks\n",
    "def check_model_assumptions(trend_results, analysis_df):\n",
    "    \"\"\"\n",
    "    Check linear model assumptions for both metrics\n",
    "    \"\"\"\n",
    "    print(\"Checking model assumptions...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Observer Effort assumptions\n",
    "    model_effort = trend_results['observer_effort']['model']\n",
    "    \n",
    "    # Residuals vs Fitted\n",
    "    axes[0, 0].scatter(model_effort.fittedvalues, model_effort.resid)\n",
    "    axes[0, 0].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[0, 0].set_xlabel('Fitted Values')\n",
    "    axes[0, 0].set_ylabel('Residuals')\n",
    "    axes[0, 0].set_title('Observer Effort: Residuals vs Fitted')\n",
    "    \n",
    "    # Q-Q plot\n",
    "    sm.qqplot(model_effort.resid, line='s', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Observer Effort: Q-Q Plot')\n",
    "    \n",
    "    # Spatial Coverage assumptions\n",
    "    model_coverage = trend_results['spatial_coverage']['model']\n",
    "    \n",
    "    # Residuals vs Fitted\n",
    "    axes[1, 0].scatter(model_coverage.fittedvalues, model_coverage.resid)\n",
    "    axes[1, 0].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[1, 0].set_xlabel('Fitted Values')\n",
    "    axes[1, 0].set_ylabel('Residuals')\n",
    "    axes[1, 0].set_title('Spatial Coverage: Residuals vs Fitted')\n",
    "    \n",
    "    # Q-Q plot\n",
    "    sm.qqplot(model_coverage.resid, line='s', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Spatial Coverage: Q-Q Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/sampling_assumptions_check.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"âœ… Assumption checks saved to ../figures/sampling_assumptions_check.png\")\n",
    "\n",
    "# Execute assumption checks\n",
    "check_model_assumptions(trend_results, analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fc576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Create Observer Effort Plot\n",
    "def create_observer_effort_plot(analysis_df):\n",
    "    \"\"\"\n",
    "    Create publication-quality plot for Observer Effort\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot main trend line\n",
    "    plt.plot(analysis_df['year'], analysis_df['observer_effort'], \n",
    "             marker='o', linewidth=2.5, markersize=8, label='Annual Effort')\n",
    "    \n",
    "    # Plot confidence intervals\n",
    "    plt.fill_between(analysis_df['year'], \n",
    "                    analysis_df['effort_lower'], \n",
    "                    analysis_df['effort_upper'],\n",
    "                    alpha=0.3, label='95% Confidence Interval')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(analysis_df['year'], analysis_df['observer_effort'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(analysis_df['year'], p(analysis_df['year']), \"r--\", \n",
    "             alpha=0.8, label=f'Trend (slope: {z[0]:.2f})')\n",
    "    \n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.ylabel('Number of Observations', fontsize=12)\n",
    "    plt.title('Observer Effort Over Time\\n(Total Observations per Year)', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add some statistics as text\n",
    "    avg_effort = analysis_df['observer_effort'].mean()\n",
    "    cv_effort = analysis_df['observer_effort'].std() / avg_effort * 100\n",
    "    plt.text(0.02, 0.98, f'Mean: {avg_effort:.0f} observations/year\\nCV: {cv_effort:.1f}%', \n",
    "             transform=plt.gca().transAxes, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/indicator_observer_effort.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"âœ… Observer effort plot saved to ../figures/indicator_observer_effort.png\")\n",
    "\n",
    "# Create the plot\n",
    "create_observer_effort_plot(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cec257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Create Spatial Coverage Plot\n",
    "def create_spatial_coverage_plot(analysis_df):\n",
    "    \"\"\"\n",
    "    Create publication-quality plot for Spatial Coverage\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot main trend line\n",
    "    plt.plot(analysis_df['year'], analysis_df['transects_surveyed'], \n",
    "             marker='s', linewidth=2.5, markersize=8, label='Transects Surveyed')\n",
    "    \n",
    "    # Plot confidence intervals\n",
    "    plt.fill_between(analysis_df['year'], \n",
    "                    analysis_df['transects_lower'], \n",
    "                    analysis_df['transects_upper'],\n",
    "                    alpha=0.3, label='95% Confidence Interval')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(analysis_df['year'], analysis_df['transects_surveyed'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(analysis_df['year'], p(analysis_df['year']), \"r--\", \n",
    "             alpha=0.8, label=f'Trend (slope: {z[0]:.2f})')\n",
    "    \n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.ylabel('Number of Transects', fontsize=12)\n",
    "    plt.title('Spatial Coverage Over Time\\n(Unique Transects Surveyed per Year)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add some statistics as text\n",
    "    total_transects = df_clean['transect_name'].nunique()\n",
    "    avg_coverage = analysis_df['transects_surveyed'].mean()\n",
    "    coverage_pct = (avg_coverage / total_transects * 100)\n",
    "    \n",
    "    plt.text(0.02, 0.98, f'Total transects in dataset: {total_transects}\\n'\n",
    "                         f'Average coverage: {coverage_pct:.1f}%', \n",
    "             transform=plt.gca().transAxes, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/indicator_spatial_coverage.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"âœ… Spatial coverage plot saved to ../figures/indicator_spatial_coverage.png\")\n",
    "\n",
    "# Create the plot\n",
    "create_spatial_coverage_plot(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a16055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Bonus Correlation Analysis\n",
    "def bonus_correlation_analysis(analysis_df):\n",
    "    \"\"\"\n",
    "    BONUS: Correlate effort with richness/abundance to show detection bias\n",
    "    \"\"\"\n",
    "    print(\"Performing bonus correlation analysis...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Correlation: Effort vs Species Richness\n",
    "    correlation_effort_richness = analysis_df['observer_effort'].corr(analysis_df['species_richness'])\n",
    "    axes[0].scatter(analysis_df['observer_effort'], analysis_df['species_richness'], s=80, alpha=0.7)\n",
    "    \n",
    "    # Add trend line\n",
    "    z_richness = np.polyfit(analysis_df['observer_effort'], analysis_df['species_richness'], 1)\n",
    "    p_richness = np.poly1d(z_richness)\n",
    "    x_range = np.linspace(analysis_df['observer_effort'].min(), analysis_df['observer_effort'].max(), 100)\n",
    "    axes[0].plot(x_range, p_richness(x_range), 'r--', alpha=0.8)\n",
    "    \n",
    "    axes[0].set_xlabel('Observer Effort (Number of Observations)')\n",
    "    axes[0].set_ylabel('Species Richness')\n",
    "    axes[0].set_title(f'Effort vs Species Richness\\n(Correlation: r = {correlation_effort_richness:.3f})')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Correlation: Spatial Coverage vs Total Abundance\n",
    "    correlation_coverage_abundance = analysis_df['transects_surveyed'].corr(analysis_df['total_abundance'])\n",
    "    axes[1].scatter(analysis_df['transects_surveyed'], analysis_df['total_abundance'], s=80, alpha=0.7)\n",
    "    \n",
    "    # Add trend line\n",
    "    z_abundance = np.polyfit(analysis_df['transects_surveyed'], analysis_df['total_abundance'], 1)\n",
    "    p_abundance = np.poly1d(z_abundance)\n",
    "    x_range = np.linspace(analysis_df['transects_surveyed'].min(), analysis_df['transects_surveyed'].max(), 100)\n",
    "    axes[1].plot(x_range, p_abundance(x_range), 'r--', alpha=0.8)\n",
    "    \n",
    "    axes[1].set_xlabel('Spatial Coverage (Number of Transects)')\n",
    "    axes[1].set_ylabel('Total Abundance')\n",
    "    axes[1].set_title(f'Coverage vs Total Abundance\\n(Correlation: r = {correlation_coverage_abundance:.3f})')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/sampling_correlation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"âœ… Correlation analysis saved to ../figures/sampling_correlation_analysis.png\")\n",
    "    \n",
    "    return {\n",
    "        'effort_richness_correlation': correlation_effort_richness,\n",
    "        'coverage_abundance_correlation': correlation_coverage_abundance\n",
    "    }\n",
    "\n",
    "# Execute correlation analysis\n",
    "correlation_results = bonus_correlation_analysis(analysis_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9765887c",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Part A - Indicator Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. FIT LINEAR TREND MODELS\n",
    "# ============================================================\n",
    "\n",
    "# Center the year variable (subtract mean year) for better numerical stability\n",
    "year_mean = df_annual['year'].mean()\n",
    "df_annual['year_centered'] = df_annual['year'] - year_mean\n",
    "\n",
    "# Fit OLS model for Species Richness\n",
    "X_richness = sm.add_constant(df_annual['year_centered'])\n",
    "y_richness = df_annual['richness']\n",
    "model_richness = sm.OLS(y_richness, X_richness).fit()\n",
    "\n",
    "# Fit OLS model for Shannon Diversity\n",
    "X_shannon = sm.add_constant(df_annual['year_centered'])\n",
    "y_shannon = df_annual['shannon']\n",
    "model_shannon = sm.OLS(y_shannon, X_shannon).fit()\n",
    "\n",
    "# Extract key statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"LINEAR TREND MODEL: Species Richness\")\n",
    "print(\"=\" * 60)\n",
    "print(model_richness.summary())\n",
    "print(\"\\nKey Statistics:\")\n",
    "print(f\"  Slope (per year): {model_richness.params['year_centered']:.4f}\")\n",
    "print(f\"  P-value: {model_richness.pvalues['year_centered']:.4f}\")\n",
    "print(f\"  R-squared: {model_richness.rsquared:.4f}\")\n",
    "print(f\"  95% CI for slope: [{model_richness.conf_int().loc['year_centered', 0]:.4f}, \"\n",
    "      f\"{model_richness.conf_int().loc['year_centered', 1]:.4f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LINEAR TREND MODEL: Shannon Diversity\")\n",
    "print(\"=\" * 60)\n",
    "print(model_shannon.summary())\n",
    "print(\"\\nKey Statistics:\")\n",
    "print(f\"  Slope (per year): {model_shannon.params['year_centered']:.4f}\")\n",
    "print(f\"  P-value: {model_shannon.pvalues['year_centered']:.4f}\")\n",
    "print(f\"  R-squared: {model_shannon.rsquared:.4f}\")\n",
    "print(f\"  95% CI for slope: [{model_shannon.conf_int().loc['year_centered', 0]:.4f}, \"\n",
    "      f\"{model_shannon.conf_int().loc['year_centered', 1]:.4f}]\")\n",
    "\n",
    "# Add fitted values and residuals to dataframe\n",
    "df_annual['richness_fitted'] = model_richness.fittedvalues\n",
    "df_annual['richness_residuals'] = model_richness.resid\n",
    "\n",
    "df_annual['shannon_fitted'] = model_shannon.fittedvalues\n",
    "df_annual['shannon_residuals'] = model_shannon.resid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e8540",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 7. INTERPRETATION PARAGRAPHS\n",
    "# ============================================================\n",
    "\n",
    "## Species Richness\n",
    "\n",
    "Species richness, measured as the number of unique species observed per year, showed a [PATTERN] trend between 2015 and 2024. The linear regression model revealed a slope of [SLOPE] species per year (p = [PVAL]), indicating [INCREASING/DECREASING/STABLE] species richness over the study period. The model explained [RÂ²]% of the variance in annual richness (RÂ² = [RÂ²]). Bootstrap-based 95% confidence intervals capture the uncertainty in annual estimates, with intervals ranging from approximately [MIN_CI] to [MAX_CI] species across years. \n",
    "\n",
    "The observed pattern suggests [ECOLOGICAL INTERPRETATION]. Potential drivers of this trend may include changes in habitat quality, sampling effort, or actual shifts in the bird community composition. The [SIGNIFICANT/NON-SIGNIFICANT] trend should be interpreted with caution given the relatively short time series and potential confounding factors such as observer effects or temporal variation in survey effort.\n",
    "\n",
    "## Shannon Diversity Index\n",
    "\n",
    "The Shannon diversity index, which accounts for both species richness and evenness (the relative abundance distribution), exhibited a [PATTERN] pattern between 2015 and 2024. The linear trend model indicated a slope of [SLOPE] units per year (p = [PVAL]), suggesting [INCREASING/DECREASING/STABLE] diversity over time. The model's RÂ² value of [RÂ²] indicates that [RÂ²]% of the variance in Shannon diversity is explained by the temporal trend.\n",
    "\n",
    "Changes in Shannon diversity reflect both the number of species present and how evenly individuals are distributed among species. [INCREASING/DECREASING] values suggest [ECOLOGICAL INTERPRETATION]. The bootstrap confidence intervals (ranging from approximately [MIN_CI] to [MAX_CI] across years) provide robust uncertainty estimates for these diversity metrics. \n",
    "\n",
    "The relationship between Shannon diversity and species richness trends can reveal whether changes are driven primarily by species gains/losses or by shifts in relative abundances. [INTERPRETATION OF RELATIONSHIP]. Potential limitations include the influence of sampling effort variation across years and the assumption that observation methods remained consistent throughout the study period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d1a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the interpretation paragraphs with actual values\n",
    "richness_slope = model_richness.params['year_centered']\n",
    "richness_pval = model_richness.pvalues['year_centered']\n",
    "richness_r2 = model_richness.rsquared\n",
    "richness_pattern = \"increasing\" if richness_slope > 0 else \"decreasing\" if richness_slope < 0 else \"stable\"\n",
    "richness_sig = \"significant\" if richness_pval < 0.05 else \"non-significant\"\n",
    "richness_min_ci = df_annual['richness_ci_lower'].min()\n",
    "richness_max_ci = df_annual['richness_ci_upper'].max()\n",
    "\n",
    "shannon_slope = model_shannon.params['year_centered']\n",
    "shannon_pval = model_shannon.pvalues['year_centered']\n",
    "shannon_r2 = model_shannon.rsquared\n",
    "shannon_pattern = \"increasing\" if shannon_slope > 0 else \"decreasing\" if shannon_slope < 0 else \"stable\"\n",
    "shannon_sig = \"significant\" if shannon_pval < 0.05 else \"non-significant\"\n",
    "shannon_min_ci = df_annual['shannon_ci_lower'].min()\n",
    "shannon_max_ci = df_annual['shannon_ci_upper'].max()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INTERPRETATION: Species Richness\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Species richness, measured as the number of unique species observed per year, showed a {richness_pattern} trend between 2015 and 2024. The linear regression model revealed a slope of {richness_slope:.3f} species per year (p = {richness_pval:.4f}), indicating a {richness_pattern} trend in species richness over the study period. The model explained {richness_r2*100:.1f}% of the variance in annual richness (RÂ² = {richness_r2:.3f}). Bootstrap-based 95% confidence intervals capture the uncertainty in annual estimates, with intervals ranging from approximately {richness_min_ci:.0f} to {richness_max_ci:.0f} species across years.\n",
    "\n",
    "The observed pattern suggests {'positive' if richness_slope > 0 else 'negative' if richness_slope < 0 else 'stable'} changes in species composition over the study period. Potential drivers of this trend may include changes in habitat quality, sampling effort, or actual shifts in the bird community composition. The {richness_sig} trend should be interpreted with caution given the relatively short time series and potential confounding factors such as observer effects or temporal variation in survey effort.\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INTERPRETATION: Shannon Diversity Index\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "The Shannon diversity index, which accounts for both species richness and evenness (the relative abundance distribution), exhibited a {shannon_pattern} pattern between 2015 and 2024. The linear trend model indicated a slope of {shannon_slope:.4f} units per year (p = {shannon_pval:.4f}), suggesting a {shannon_pattern} trend in diversity over time. The model's RÂ² value of {shannon_r2:.3f} indicates that {shannon_r2*100:.1f}% of the variance in Shannon diversity is explained by the temporal trend.\n",
    "\n",
    "Changes in Shannon diversity reflect both the number of species present and how evenly individuals are distributed among species. {'Increasing' if shannon_slope > 0 else 'Decreasing' if shannon_slope < 0 else 'Stable'} values suggest {'improving' if shannon_slope > 0 else 'declining' if shannon_slope < 0 else 'stable'} community diversity and evenness. The bootstrap confidence intervals (ranging from approximately {shannon_min_ci:.3f} to {shannon_max_ci:.3f} across years) provide robust uncertainty estimates for these diversity metrics.\n",
    "\n",
    "The relationship between Shannon diversity and species richness trends can reveal whether changes are driven primarily by species gains/losses or by shifts in relative abundances. {'Both metrics show similar' if (richness_slope > 0) == (shannon_slope > 0) else 'The metrics show divergent'} trends, suggesting {'changes are primarily driven by species-level dynamics' if abs(richness_slope) > abs(shannon_slope) else 'changes involve both species composition and abundance distribution'}. Potential limitations include the influence of sampling effort variation across years and the assumption that observation methods remained consistent throughout the study period.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8. SAVE RESULTS\n",
    "# ============================================================\n",
    "\n",
    "# Export annual indicators to CSV\n",
    "output_cols = ['year', 'richness', 'richness_ci_lower', 'richness_ci_upper',\n",
    "               'shannon', 'shannon_ci_lower', 'shannon_ci_upper', 'n_records']\n",
    "df_annual[output_cols].to_csv('../data/annual_indicators_2015_2024.csv', index=False)\n",
    "print(\"Saved annual indicators: ../data/annual_indicators_2015_2024.csv\")\n",
    "\n",
    "# Export regression summaries to text files\n",
    "with open('../data/regression_summary_richness.txt', 'w') as f:\n",
    "    f.write(\"Species Richness - Linear Trend Model Summary\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(str(model_richness.summary()))\n",
    "    f.write(\"\\n\\nBootstrap Statistics:\\n\")\n",
    "    f.write(f\"  Mean richness: {df_annual['richness'].mean():.2f}\\n\")\n",
    "    f.write(f\"  SD richness: {df_annual['richness'].std():.2f}\\n\")\n",
    "    f.write(f\"  CI range: {richness_min_ci:.0f} - {richness_max_ci:.0f}\\n\")\n",
    "\n",
    "with open('../data/regression_summary_shannon.txt', 'w') as f:\n",
    "    f.write(\"Shannon Diversity - Linear Trend Model Summary\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(str(model_shannon.summary()))\n",
    "    f.write(\"\\n\\nBootstrap Statistics:\\n\")\n",
    "    f.write(f\"  Mean Shannon: {df_annual['shannon'].mean():.3f}\\n\")\n",
    "    f.write(f\"  SD Shannon: {df_annual['shannon'].std():.3f}\\n\")\n",
    "    f.write(f\"  CI range: {shannon_min_ci:.3f} - {shannon_max_ci:.3f}\\n\")\n",
    "\n",
    "print(\"Saved regression summaries:\")\n",
    "print(\"  - regression_summary_richness.txt\")\n",
    "print(\"  - regression_summary_shannon.txt\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SECTION 2 COMPLETE - All deliverables saved!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  âœ“ figures/indicator_species_richness.png\")\n",
    "print(\"  âœ“ figures/indicator_shannon_diversity.png\")\n",
    "print(\"  âœ“ figures/indicator_model_diagnostics.png\")\n",
    "print(\"  âœ“ data/annual_indicators_2015_2024.csv\")\n",
    "print(\"  âœ“ data/regression_summary_richness.txt\")\n",
    "print(\"  âœ“ data/regression_summary_shannon.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ca6388",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Part B - Indicators (Oliver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9933a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define functions for computing abundance metrics\n",
    "\"\"\"\n",
    "\n",
    "def compute_mean_abundance_per_point(df):\n",
    "    \"\"\"\n",
    "    Compute mean abundance per observation point.\n",
    "    \n",
    "    This standardizes for sampling effort by dividing total abundance\n",
    "    by the number of unique observation points visited.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Observations with 'individual_count', 'transect_name', 'point_number'\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean abundance per point\n",
    "    \"\"\"\n",
    "    # Count unique observation points (transect-point combinations)\n",
    "    n_points = df[['transect_name', 'point_number']].drop_duplicates().shape[0]\n",
    "    \n",
    "    # Total abundance\n",
    "    total_abundance = df['individual_count'].sum()\n",
    "    \n",
    "    # Mean per point\n",
    "    if n_points > 0:\n",
    "        return total_abundance / n_points\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Test the function\n",
    "test_year = df_complete[df_complete['year'] == 2020]\n",
    "test_total = test_year['individual_count'].sum()\n",
    "test_mean = compute_mean_abundance_per_point(test_year)\n",
    "\n",
    "print(\"Function test (2020):\")\n",
    "print(f\"  Total abundance: {test_total:,.0f}\")\n",
    "print(f\"  Mean per point: {test_mean:.2f}\")\n",
    "print(\"âœ“ Functions defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06122eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute both indicators for each year (2015-2024)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPUTING ANNUAL INDICATORS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "years = sorted(df_complete['year'].unique())\n",
    "\n",
    "# Initialize results\n",
    "results = []\n",
    "\n",
    "for year in years:\n",
    "    year_data = df_complete[df_complete['year'] == year]\n",
    "    \n",
    "    # Indicator 1: Total Abundance\n",
    "    total_abundance = year_data['individual_count'].sum()\n",
    "    \n",
    "    # Indicator 2: Mean Abundance per Point\n",
    "    mean_abundance_per_point = compute_mean_abundance_per_point(year_data)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'year': year,\n",
    "        'total_abundance': total_abundance,\n",
    "        'mean_abundance_per_point': mean_abundance_per_point,\n",
    "        'n_observations': len(year_data),\n",
    "        'n_points': year_data[['transect_name', 'point_number']].drop_duplicates().shape[0]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{year}:\")\n",
    "    print(f\"  Total Abundance: {total_abundance:>10,.0f} individuals\")\n",
    "    print(f\"  Mean per Point:  {mean_abundance_per_point:>10,.2f} individuals/point\")\n",
    "    print(f\"  Observations:    {len(year_data):>10,}\")\n",
    "    print(f\"  Points surveyed: {year_data[['transect_name', 'point_number']].drop_duplicates().shape[0]:>10,}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTotal Abundance:\")\n",
    "print(results_df['total_abundance'].describe())\n",
    "print(\"\\nMean Abundance per Point:\")\n",
    "print(results_df['mean_abundance_per_point'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c197f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export results to CSV for team integration\n",
    "\"\"\"\n",
    "\n",
    "# Combine both indicators into one DataFrame\n",
    "export_data = []\n",
    "\n",
    "# Total Abundance\n",
    "for _, row in total_abundance_ci_df.iterrows():\n",
    "    export_data.append({\n",
    "        'year': row['year'],\n",
    "        'indicator_name': 'Total Abundance',\n",
    "        'estimate': row['estimate'],\n",
    "        'ci_lower': row['ci_lower'],\n",
    "        'ci_upper': row['ci_upper'],\n",
    "        'stderr': row['stderr']\n",
    "    })\n",
    "\n",
    "# Mean Abundance per Point\n",
    "for _, row in mean_abundance_ci_df.iterrows():\n",
    "    export_data.append({\n",
    "        'year': row['year'],\n",
    "        'indicator_name': 'Mean Abundance per Point',\n",
    "        'estimate': row['estimate'],\n",
    "        'ci_lower': row['ci_lower'],\n",
    "        'ci_upper': row['ci_upper'],\n",
    "        'stderr': row['stderr']\n",
    "    })\n",
    "\n",
    "export_df = pd.DataFrame(export_data)\n",
    "\n",
    "# Save to CSV\n",
    "export_df.to_csv('../results/indicators_person_B.csv', index=False)\n",
    "print(\"âœ“ Results exported to ../results/indicators_person_B.csv\")\n",
    "print(\"\\nPreview:\")\n",
    "print(export_df.head(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df4d61",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Part C - Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afbd619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Calculate Sampling Metrics\n",
    "def calculate_sampling_metrics(df_clean):\n",
    "    \"\"\"\n",
    "    Calculate annual sampling metrics for Person C\n",
    "    \"\"\"\n",
    "    print(\"Calculating sampling metrics...\")\n",
    "    \n",
    "    # Filter to complete years (2015-2024 as specified)\n",
    "    df_complete = df_clean[df_clean['year'].between(2015, 2024)].copy()\n",
    "    \n",
    "    annual_metrics = []\n",
    "    \n",
    "    for year in sorted(df_complete['year'].unique()):\n",
    "        year_data = df_complete[df_complete['year'] == year]\n",
    "        \n",
    "        # Calculate metrics for this year\n",
    "        metrics = {\n",
    "            'year': year,\n",
    "            \n",
    "            # Observer Effort: Total number of observations\n",
    "            'observer_effort': len(year_data),\n",
    "            \n",
    "            # Spatial Coverage: Unique transects surveyed\n",
    "            'transects_surveyed': year_data['transect_name'].nunique(),\n",
    "            \n",
    "            # Additional metrics for context\n",
    "            'n_observers': year_data['observer_name'].nunique(),\n",
    "            'total_abundance': year_data['individual_count'].sum(),\n",
    "            'species_richness': year_data['species_name'].nunique()\n",
    "        }\n",
    "        \n",
    "        annual_metrics.append(metrics)\n",
    "    \n",
    "    metrics_df = pd.DataFrame(annual_metrics)\n",
    "    \n",
    "    # Calculate spatial coverage as percentage (if we know total possible transects)\n",
    "    # For now, we'll use absolute number of transects, or calculate coverage rate\n",
    "    total_transects_overall = df_clean['transect_name'].nunique()\n",
    "    metrics_df['spatial_coverage_pct'] = (metrics_df['transects_surveyed'] / total_transects_overall * 100)\n",
    "    \n",
    "    print(f\"Total unique transects in dataset: {total_transects_overall}\")\n",
    "    print(f\"Years analyzed: {len(metrics_df)} years from {metrics_df['year'].min()} to {metrics_df['year'].max()}\")\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Execute the function\n",
    "metrics_df = calculate_sampling_metrics(df_clean)\n",
    "print(\"\\nðŸ“ˆ Sampling metrics calculated:\")\n",
    "print(metrics_df[['year', 'observer_effort', 'transects_surveyed']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d490fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Trend Analysis\n",
    "def analyze_sampling_trends(metrics_df, bootstrap_df):\n",
    "    \"\"\"\n",
    "    Analyze temporal trends in sampling metrics using linear models\n",
    "    \"\"\"\n",
    "    print(\"Analyzing temporal trends...\")\n",
    "    \n",
    "    # Merge metrics with bootstrap results\n",
    "    analysis_df = metrics_df.merge(bootstrap_df, on='year')\n",
    "    \n",
    "    trend_results = {}\n",
    "    \n",
    "    # 1. Observer Effort trend analysis\n",
    "    X_effort = analysis_df['year'].values\n",
    "    X_effort = sm.add_constant(X_effort)  # Add intercept\n",
    "    y_effort = analysis_df['observer_effort'].values\n",
    "    \n",
    "    model_effort = sm.OLS(y_effort, X_effort).fit()\n",
    "    trend_results['observer_effort'] = {\n",
    "        'model': model_effort,\n",
    "        'slope': model_effort.params[1],\n",
    "        'slope_pvalue': model_effort.pvalues[1],\n",
    "        'r_squared': model_effort.rsquared,\n",
    "        'trend_direction': 'increasing' if model_effort.params[1] > 0 else 'decreasing'\n",
    "    }\n",
    "    \n",
    "    # 2. Spatial Coverage trend analysis\n",
    "    X_coverage = analysis_df['year'].values\n",
    "    X_coverage = sm.add_constant(X_coverage)\n",
    "    y_coverage = analysis_df['transects_surveyed'].values\n",
    "    \n",
    "    model_coverage = sm.OLS(y_coverage, X_coverage).fit()\n",
    "    trend_results['spatial_coverage'] = {\n",
    "        'model': model_coverage,\n",
    "        'slope': model_coverage.params[1],\n",
    "        'slope_pvalue': model_coverage.pvalues[1],\n",
    "        'r_squared': model_coverage.rsquared,\n",
    "        'trend_direction': 'increasing' if model_coverage.params[1] > 0 else 'decreasing'\n",
    "    }\n",
    "    \n",
    "    return analysis_df, trend_results\n",
    "\n",
    "# Execute trend analysis\n",
    "analysis_df, trend_results = analyze_sampling_trends(metrics_df, bootstrap_df)\n",
    "\n",
    "print(\"âœ… Trend analysis completed:\")\n",
    "print(f\"Observer Effort - Slope: {trend_results['observer_effort']['slope']:.3f}, \"\n",
    "      f\"p-value: {trend_results['observer_effort']['slope_pvalue']:.3f}, \"\n",
    "      f\"RÂ²: {trend_results['observer_effort']['r_squared']:.3f}\")\n",
    "\n",
    "print(f\"Spatial Coverage - Slope: {trend_results['spatial_coverage']['slope']:.3f}, \"\n",
    "      f\"p-value: {trend_results['spatial_coverage']['slope_pvalue']:.3f}, \"\n",
    "      f\"RÂ²: {trend_results['spatial_coverage']['r_squared']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92b883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Write Interpretation and Save Results\n",
    "def write_interpretation(trend_results, correlation_results, analysis_df):\n",
    "    \"\"\"\n",
    "    Write interpretation paragraphs for both indicators\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INTERPRETATION FOR PERSON C - SAMPLING METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Observer Effort Interpretation\n",
    "    effort_trend = trend_results['observer_effort']\n",
    "    print(\"\\nðŸ“Š OBSERVER EFFORT INTERPRETATION:\")\n",
    "    print(f\"Observer effort, measured as the total number of observations per year, \")\n",
    "    print(f\"showed a {effort_trend['trend_direction']} trend from 2015 to 2024.\")\n",
    "    print(f\"The linear model indicated a slope of {effort_trend['slope']:.2f} observations per year \")\n",
    "    print(f\"(p-value: {effort_trend['slope_pvalue']:.3f}, RÂ²: {effort_trend['r_squared']:.3f}).\")\n",
    "    \n",
    "    if effort_trend['slope_pvalue'] < 0.05:\n",
    "        print(\"This trend is statistically significant, suggesting a real change in monitoring intensity over time.\")\n",
    "    else:\n",
    "        print(\"This trend is not statistically significant, indicating relatively stable observer effort across years.\")\n",
    "    \n",
    "    effort_range = analysis_df['observer_effort'].max() - analysis_df['observer_effort'].min()\n",
    "    print(f\"Observer effort ranged from {analysis_df['observer_effort'].min():.0f} to {analysis_df['observer_effort'].max():.0f} \")\n",
    "    print(f\"observations per year, representing a {effort_range/analysis_df['observer_effort'].mean()*100:.1f}% variation around the mean.\")\n",
    "    \n",
    "    # Spatial Coverage Interpretation\n",
    "    coverage_trend = trend_results['spatial_coverage']\n",
    "    print(\"\\nðŸ—ºï¸ SPATIAL COVERAGE INTERPRETATION:\")\n",
    "    print(f\"Spatial coverage, measured as the number of unique transects surveyed per year, \")\n",
    "    print(f\"showed a {coverage_trend['trend_direction']} trend from 2015 to 2024.\")\n",
    "    print(f\"The linear model indicated a slope of {coverage_trend['slope']:.2f} transects per year \")\n",
    "    print(f\"(p-value: {coverage_trend['slope_pvalue']:.3f}, RÂ²: {coverage_trend['r_squared']:.3f}).\")\n",
    "    \n",
    "    if coverage_trend['slope_pvalue'] < 0.05:\n",
    "        print(\"This trend is statistically significant, indicating meaningful changes in geographic sampling extent.\")\n",
    "    else:\n",
    "        print(\"This trend is not statistically significant, suggesting consistent spatial coverage across years.\")\n",
    "    \n",
    "    total_transects = df_clean['transect_name'].nunique()\n",
    "    avg_coverage_pct = (analysis_df['transects_surveyed'].mean() / total_transects * 100)\n",
    "    print(f\"On average, {avg_coverage_pct:.1f}% of all known transects were surveyed each year.\")\n",
    "    \n",
    "    # Bonus Correlation Interpretation\n",
    "    print(\"\\nðŸ” BONUS: DETECTION BIAS ANALYSIS:\")\n",
    "    print(f\"Observer effort showed a correlation of r = {correlation_results['effort_richness_correlation']:.3f} with species richness.\")\n",
    "    if abs(correlation_results['effort_richness_correlation']) > 0.5:\n",
    "        print(\"This strong correlation suggests potential detection bias - years with higher effort may record more species.\")\n",
    "    else:\n",
    "        print(\"This weak correlation suggests that species richness estimates are relatively independent of observer effort.\")\n",
    "    \n",
    "    print(f\"Spatial coverage showed a correlation of r = {correlation_results['coverage_abundance_correlation']:.3f} with total abundance.\")\n",
    "    if abs(correlation_results['coverage_abundance_correlation']) > 0.5:\n",
    "        print(\"This indicates that abundance estimates may be influenced by the number of transects surveyed.\")\n",
    "    else:\n",
    "        print(\"This suggests that abundance patterns are robust to variations in spatial coverage.\")\n",
    "\n",
    "# Ensure directories exist\n",
    "Path('figures').mkdir(exist_ok=True)\n",
    "Path('projects/birds-biodiversity/results').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Write interpretations\n",
    "write_interpretation(trend_results, correlation_results, analysis_df)\n",
    "\n",
    "# Save results table\n",
    "results_table = analysis_df[['year', 'observer_effort', 'effort_lower', 'effort_upper', \n",
    "                           'transects_surveyed', 'transects_lower', 'transects_upper']]\n",
    "results_table.to_csv('projects/birds-biodiversity/results/personC_sampling_metrics.csv', index=False)\n",
    "print(f\"\\nâœ… Results table saved to projects/birds-biodiversity/results/personC_sampling_metrics.csv\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ PERSON C ANALYSIS COMPLETE!\")\n",
    "print(\"Generated files:\")\n",
    "print(\"  - figures/indicator_observer_effort.png\")\n",
    "print(\"  - figures/indicator_spatial_coverage.png\") \n",
    "print(\"  - figures/sampling_assumptions_check.png\")\n",
    "print(\"  - figures/sampling_correlation_analysis.png\")\n",
    "print(\"  - results/personC_sampling_metrics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
